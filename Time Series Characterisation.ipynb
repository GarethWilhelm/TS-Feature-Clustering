{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><b><font size=\"5\">Time Series Analysis and Forecasting in Python</font></b></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Time-Series\" data-toc-modified-id=\"Time-Series-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Time Series</a></span></li><li><span><a href=\"#Business-use-cases-of-time-series-forecasting\" data-toc-modified-id=\"Business-use-cases-of-time-series-forecasting-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Business use cases of time series forecasting</a></span></li><li><span><a href=\"#Importing-Libraries-for-time-series-forecasting\" data-toc-modified-id=\"Importing-Libraries-for-time-series-forecasting-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Importing Libraries for time series forecasting</a></span></li><li><span><a href=\"#Importing-data\" data-toc-modified-id=\"Importing-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Importing data</a></span></li><li><span><a href=\"#Data-Preprocessing-and-Visualization\" data-toc-modified-id=\"Data-Preprocessing-and-Visualization-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Data Preprocessing and Visualization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stationarity\" data-toc-modified-id=\"Stationarity-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Stationarity</a></span><ul class=\"toc-item\"><li><span><a href=\"#ACF-and-PACF-plots\" data-toc-modified-id=\"ACF-and-PACF-plots-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>ACF and PACF plots</a></span></li><li><span><a href=\"#Plotting-Rolling-Statistics\" data-toc-modified-id=\"Plotting-Rolling-Statistics-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Plotting Rolling Statistics</a></span></li><li><span><a href=\"#Augmented-Dickey-Fuller-Test\" data-toc-modified-id=\"Augmented-Dickey-Fuller-Test-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Augmented Dickey-Fuller Test</a></span></li></ul></li><li><span><a href=\"#Making-Time-Series-Stationary\" data-toc-modified-id=\"Making-Time-Series-Stationary-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Making Time Series Stationary</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transformations\" data-toc-modified-id=\"Transformations-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Transformations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Log-Scale-Transformation\" data-toc-modified-id=\"Log-Scale-Transformation-5.2.1.1\"><span class=\"toc-item-num\">5.2.1.1&nbsp;&nbsp;</span>Log Scale Transformation</a></span></li><li><span><a href=\"#Other-possible-transformations:\" data-toc-modified-id=\"Other-possible-transformations:-5.2.1.2\"><span class=\"toc-item-num\">5.2.1.2&nbsp;&nbsp;</span>Other possible transformations:</a></span></li></ul></li><li><span><a href=\"#Techniques-to-remove-Trend---Smoothing\" data-toc-modified-id=\"Techniques-to-remove-Trend---Smoothing-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Techniques to remove Trend - Smoothing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Moving-Average\" data-toc-modified-id=\"Moving-Average-5.2.2.1\"><span class=\"toc-item-num\">5.2.2.1&nbsp;&nbsp;</span>Moving Average</a></span></li><li><span><a href=\"#Exponentially-weighted-moving-average:\" data-toc-modified-id=\"Exponentially-weighted-moving-average:-5.2.2.2\"><span class=\"toc-item-num\">5.2.2.2&nbsp;&nbsp;</span>Exponentially weighted moving average:</a></span></li></ul></li><li><span><a href=\"#Further-Techniques-to-remove-Seasonality-and-Trend\" data-toc-modified-id=\"Further-Techniques-to-remove-Seasonality-and-Trend-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Further Techniques to remove Seasonality and Trend</a></span><ul class=\"toc-item\"><li><span><a href=\"#Differencing\" data-toc-modified-id=\"Differencing-5.2.3.1\"><span class=\"toc-item-num\">5.2.3.1&nbsp;&nbsp;</span>Differencing</a></span></li><li><span><a href=\"#Decomposition\" data-toc-modified-id=\"Decomposition-5.2.3.2\"><span class=\"toc-item-num\">5.2.3.2&nbsp;&nbsp;</span>Decomposition</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Time-Series-forecasting\" data-toc-modified-id=\"Time-Series-forecasting-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Time Series forecasting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Autoregression-(AR)\" data-toc-modified-id=\"Autoregression-(AR)-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Autoregression (AR)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Reversing-the-transformations\" data-toc-modified-id=\"Reversing-the-transformations-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Reversing the transformations</a></span></li><li><span><a href=\"#Forecast-quality-scoring-metrics\" data-toc-modified-id=\"Forecast-quality-scoring-metrics-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Forecast quality scoring metrics</a></span></li></ul></li><li><span><a href=\"#Moving-Average-(MA)\" data-toc-modified-id=\"Moving-Average-(MA)-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Moving Average (MA)</a></span></li><li><span><a href=\"#Autoregressive-Moving-Average-(ARMA)\" data-toc-modified-id=\"Autoregressive-Moving-Average-(ARMA)-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Autoregressive Moving Average (ARMA)</a></span></li><li><span><a href=\"#Autoregressive-Integrated-Moving-Average-(ARIMA)\" data-toc-modified-id=\"Autoregressive-Integrated-Moving-Average-(ARIMA)-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Autoregressive Integrated Moving Average (ARIMA)</a></span></li><li><span><a href=\"#Interpreting-ACF-plots\" data-toc-modified-id=\"Interpreting-ACF-plots-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Interpreting ACF plots</a></span><ul class=\"toc-item\"><li><span><a href=\"#Auto-ARIMA\" data-toc-modified-id=\"Auto-ARIMA-6.5.1\"><span class=\"toc-item-num\">6.5.1&nbsp;&nbsp;</span>Auto ARIMA</a></span></li></ul></li><li><span><a href=\"#Seasonal-Autoregressive-Integrated-Moving-Average-(SARIMA)\" data-toc-modified-id=\"Seasonal-Autoregressive-Integrated-Moving-Average-(SARIMA)-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Seasonal Autoregressive Integrated Moving-Average (SARIMA)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Auto---SARIMA\" data-toc-modified-id=\"Auto---SARIMA-6.6.1\"><span class=\"toc-item-num\">6.6.1&nbsp;&nbsp;</span>Auto - SARIMA</a></span></li><li><span><a href=\"#Tuned-SARIMA\" data-toc-modified-id=\"Tuned-SARIMA-6.6.2\"><span class=\"toc-item-num\">6.6.2&nbsp;&nbsp;</span>Tuned SARIMA</a></span></li></ul></li><li><span><a href=\"#SARIMAX\" data-toc-modified-id=\"SARIMAX-6.7\"><span class=\"toc-item-num\">6.7&nbsp;&nbsp;</span>SARIMAX</a></span></li><li><span><a href=\"#Prophet\" data-toc-modified-id=\"Prophet-6.8\"><span class=\"toc-item-num\">6.8&nbsp;&nbsp;</span>Prophet</a></span></li><li><span><a href=\"#Improving-Time-Series-Forecast-models\" data-toc-modified-id=\"Improving-Time-Series-Forecast-models-6.9\"><span class=\"toc-item-num\">6.9&nbsp;&nbsp;</span>Improving Time Series Forecast models</a></span></li><li><span><a href=\"#Solve-a-problem!\" data-toc-modified-id=\"Solve-a-problem!-6.10\"><span class=\"toc-item-num\">6.10&nbsp;&nbsp;</span>Solve a problem!</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series\n",
    "- A time series is a sequential set of data points, measured typically over successive times. \n",
    "- It is mathematically defined as a set of vectors x(t), t = 0,1,2,... where t represents the time elapsed. \n",
    "- The variable x(t) is treated as a random variable. \n",
    "- The measurements taken during an event in a time series are arranged in a proper chronological order. \n",
    "- A time series containing records of a single variable is termed as univariate, and more than one variable a multivariate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business use cases of time series forecasting\n",
    "- __Sales forecast - retail products sales forecast__\n",
    "- __Demand forecasting - used in pricing, inventory and workforce management__\n",
    "- __Traffic forecast - transport and route optimization, road facility design__\n",
    "- __Revenue forecast - budgeting, target setting__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries for time series forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "from statsmodels.tsa.arima_model import ARMA, ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "#from pyramid.arima import auto_arima\n",
    "#from fbprophet import Prophet\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "matplotlib.rcParams['xtick.labelsize'] = 12\n",
    "matplotlib.rcParams['ytick.labelsize'] = 12\n",
    "matplotlib.rcParams['text.color'] = 'k'\n",
    "import seaborn as sns\n",
    "\n",
    "from random import random\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error, mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data\n",
    "- Dataset: International airline passengers\n",
    "- Unit: Thousands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>year</th>\n",
       "      <th>export_value</th>\n",
       "      <th>import_value</th>\n",
       "      <th>export_rca</th>\n",
       "      <th>is_new</th>\n",
       "      <th>cog</th>\n",
       "      <th>distance</th>\n",
       "      <th>normalized_distance</th>\n",
       "      <th>normalized_cog</th>\n",
       "      <th>normalized_pci</th>\n",
       "      <th>export_rpop</th>\n",
       "      <th>sitc_eci</th>\n",
       "      <th>sitc_coi</th>\n",
       "      <th>pci</th>\n",
       "      <th>location_code</th>\n",
       "      <th>location_name_short_en</th>\n",
       "      <th>sitc_product_code</th>\n",
       "      <th>sitc_product_name_short_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>650</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.981369</td>\n",
       "      <td>-0.557001</td>\n",
       "      <td>-0.208299</td>\n",
       "      <td>-0.320203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.398989</td>\n",
       "      <td>-0.276065</td>\n",
       "      <td>-0.323631</td>\n",
       "      <td>ABW</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>11</td>\n",
       "      <td>Animals of the bovine species (including buffa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>650</td>\n",
       "      <td>1989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.961464</td>\n",
       "      <td>-0.273027</td>\n",
       "      <td>-0.466872</td>\n",
       "      <td>-0.664214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.894513</td>\n",
       "      <td>-0.227471</td>\n",
       "      <td>-0.655982</td>\n",
       "      <td>ABW</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>11</td>\n",
       "      <td>Animals of the bovine species (including buffa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   location_id  product_id  year  export_value  import_value  export_rca  is_new       cog  distance  normalized_distance  normalized_cog  normalized_pci  export_rpop  sitc_eci  sitc_coi       pci location_code location_name_short_en sitc_product_code                         sitc_product_name_short_en\n",
       "0            0         650  1988           0.0           0.0         0.0   False  0.002104  0.981369            -0.557001       -0.208299       -0.320203          0.0  1.398989 -0.276065 -0.323631           ABW                  Aruba                11  Animals of the bovine species (including buffa...\n",
       "1            0         650  1989           0.0           0.0         0.0   False  0.001324  0.961464            -0.273027       -0.466872       -0.664214          0.0  0.894513 -0.227471 -0.655982           ABW                  Aruba                11  Animals of the bovine species (including buffa..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('sitc4digit_year.csv')#,header=None)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['location_id', 'product_id', 'year', 'export_value', 'import_value', 'export_rca', 'is_new', 'cog', 'distance', 'normalized_distance', 'normalized_cog', 'normalized_pci', 'export_rpop', 'sitc_eci', 'sitc_coi', 'pci', 'location_code', 'location_name_short_en', 'sitc_product_code', 'sitc_product_name_short_en'], dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>year</th>\n",
       "      <th>export_value</th>\n",
       "      <th>import_value</th>\n",
       "      <th>export_rca</th>\n",
       "      <th>is_new</th>\n",
       "      <th>cog</th>\n",
       "      <th>distance</th>\n",
       "      <th>normalized_distance</th>\n",
       "      <th>normalized_cog</th>\n",
       "      <th>normalized_pci</th>\n",
       "      <th>export_rpop</th>\n",
       "      <th>sitc_eci</th>\n",
       "      <th>sitc_coi</th>\n",
       "      <th>pci</th>\n",
       "      <th>location_code</th>\n",
       "      <th>location_name_short_en</th>\n",
       "      <th>sitc_product_code</th>\n",
       "      <th>sitc_product_name_short_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6133552</th>\n",
       "      <td>0</td>\n",
       "      <td>1190</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.007744</td>\n",
       "      <td>0.979954</td>\n",
       "      <td>-0.172193</td>\n",
       "      <td>1.210088</td>\n",
       "      <td>1.173727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.398989</td>\n",
       "      <td>-0.276065</td>\n",
       "      <td>1.171135</td>\n",
       "      <td>ABW</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>7132</td>\n",
       "      <td>Motor vehicles piston engines, headings: 722; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6133553</th>\n",
       "      <td>0</td>\n",
       "      <td>1190</td>\n",
       "      <td>1989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.008663</td>\n",
       "      <td>0.965579</td>\n",
       "      <td>-0.867740</td>\n",
       "      <td>1.374990</td>\n",
       "      <td>1.249889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.894513</td>\n",
       "      <td>-0.227471</td>\n",
       "      <td>1.261243</td>\n",
       "      <td>ABW</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>7132</td>\n",
       "      <td>Motor vehicles piston engines, headings: 722; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6133554</th>\n",
       "      <td>0</td>\n",
       "      <td>1190</td>\n",
       "      <td>1990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40625.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.009165</td>\n",
       "      <td>0.985775</td>\n",
       "      <td>-0.422732</td>\n",
       "      <td>1.462073</td>\n",
       "      <td>1.458605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.373334</td>\n",
       "      <td>-0.707455</td>\n",
       "      <td>1.462682</td>\n",
       "      <td>ABW</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>7132</td>\n",
       "      <td>Motor vehicles piston engines, headings: 722; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         location_id  product_id  year  export_value  import_value  export_rca  is_new       cog  distance  normalized_distance  normalized_cog  normalized_pci  export_rpop  sitc_eci  sitc_coi       pci location_code location_name_short_en sitc_product_code                         sitc_product_name_short_en\n",
       "6133552            0        1190  1988           0.0        1768.0         0.0   False  0.007744  0.979954            -0.172193        1.210088        1.173727          NaN  1.398989 -0.276065  1.171135           ABW                  Aruba              7132  Motor vehicles piston engines, headings: 722; ...\n",
       "6133553            0        1190  1989           0.0           0.0         0.0   False  0.008663  0.965579            -0.867740        1.374990        1.249889          NaN  0.894513 -0.227471  1.261243           ABW                  Aruba              7132  Motor vehicles piston engines, headings: 722; ...\n",
       "6133554            0        1190  1990           0.0       40625.0         0.0   False  0.009165  0.985775            -0.422732        1.462073        1.458605          NaN -0.373334 -0.707455  1.462682           ABW                  Aruba              7132  Motor vehicles piston engines, headings: 722; ..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.loc[data['sitc_product_code']==7132]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>location_name_short_en</th>\n",
       "      <th>export_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6145069</th>\n",
       "      <td>1972</td>\n",
       "      <td>Fmr Dem. Rep. of Vietnam</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6145070</th>\n",
       "      <td>1973</td>\n",
       "      <td>Fmr Dem. Rep. of Vietnam</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6145071</th>\n",
       "      <td>1974</td>\n",
       "      <td>Fmr Dem. Rep. of Vietnam</td>\n",
       "      <td>124.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         year    location_name_short_en  export_value\n",
       "6145069  1972  Fmr Dem. Rep. of Vietnam          0.00\n",
       "6145070  1973  Fmr Dem. Rep. of Vietnam          0.00\n",
       "6145071  1974  Fmr Dem. Rep. of Vietnam        124.25"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['year','location_name_short_en','export_value']]\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby(['location_name_short_en'])\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.index = df['year']\n",
    "#df.drop(['year'],axis=1,inplace=True)\n",
    "#df.head(3)\n",
    "#pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns = ['year','location_name_short_en','export_value','sitc_product_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>export_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11520.000000</td>\n",
       "      <td>1.152000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1991.288715</td>\n",
       "      <td>1.138341e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.963447</td>\n",
       "      <td>6.513351e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1962.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1978.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1992.000000</td>\n",
       "      <td>6.969375e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2005.000000</td>\n",
       "      <td>2.987648e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>1.019213e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               year  export_value\n",
       "count  11520.000000  1.152000e+04\n",
       "mean    1991.288715  1.138341e+08\n",
       "std       15.963447  6.513351e+08\n",
       "min     1962.000000  0.000000e+00\n",
       "25%     1978.000000  0.000000e+00\n",
       "50%     1992.000000  6.969375e+03\n",
       "75%     2005.000000  2.987648e+05\n",
       "max     2017.000000  1.019213e+10"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_name_short_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       location_name_short_en\n",
       "count                   11520\n",
       "unique                    250\n",
       "top                     Spain\n",
       "freq                       56"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time period start: 1962\n",
      "Time period end: year                             2017\n",
      "location_name_short_en       Zimbabwe\n",
      "export_value              1.01921e+10\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('Time period start: {}\\nTime period end: {}'.format(df.year.min(),df.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'location_name_short_en', 'export_value'], dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Converting to datetime format:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = pd.to_datetime(df['year'], format='%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Setting index as the datetime column for easier manipulations:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_name_short_en</th>\n",
       "      <th>export_value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1988-01-01</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-01-01</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-01</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           location_name_short_en  export_value\n",
       "year                                           \n",
       "1988-01-01                  Aruba           0.0\n",
       "1989-01-01                  Aruba           0.0\n",
       "1990-01-01                  Aruba           0.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.set_index('year')\n",
    "y.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['1988-01-01', '1989-01-01', '1990-01-01', '1991-01-01', '1992-01-01', '1993-01-01', '1994-01-01', '1995-01-01', '1996-01-01', '1997-01-01',\n",
       "               ...\n",
       "               '1965-01-01', '1966-01-01', '1967-01-01', '1968-01-01', '1969-01-01', '1970-01-01', '1971-01-01', '1972-01-01', '1973-01-01', '1974-01-01'], dtype='datetime64[ns]', name='year', length=11520, freq=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location_name_short_en    0\n",
       "export_value              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_name_short_en</th>\n",
       "      <th>export_value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1988-01-01</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-01-01</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-01</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-01-01</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-01-01</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-01-01</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-01-01</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-01</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-01-01</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-01-01</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-01-01</th>\n",
       "      <td>Angola</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-01-01</th>\n",
       "      <td>Angola</td>\n",
       "      <td>1317.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-01</th>\n",
       "      <td>Angola</td>\n",
       "      <td>1597.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-01-01</th>\n",
       "      <td>Angola</td>\n",
       "      <td>5947.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-01-01</th>\n",
       "      <td>Angola</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-01-01</th>\n",
       "      <td>Anguilla</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-01-01</th>\n",
       "      <td>Anguilla</td>\n",
       "      <td>572.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-01</th>\n",
       "      <td>Anguilla</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-01-01</th>\n",
       "      <td>Anguilla</td>\n",
       "      <td>11258.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-01-01</th>\n",
       "      <td>Anguilla</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-01-01</th>\n",
       "      <td>Albania</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-01-01</th>\n",
       "      <td>Albania</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-01</th>\n",
       "      <td>Albania</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-01-01</th>\n",
       "      <td>Albania</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-01-01</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1088.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-01-01</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>6220.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-01-01</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>3206.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-01</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>6607.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-01-01</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>2894.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-01-01</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>17287.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01</th>\n",
       "      <td>South Georgia and South Sandwich Islds.</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-01</th>\n",
       "      <td>South Georgia and South Sandwich Islds.</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-01</th>\n",
       "      <td>South Georgia and South Sandwich Islds.</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-01</th>\n",
       "      <td>South Georgia and South Sandwich Islds.</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-01</th>\n",
       "      <td>South Georgia and South Sandwich Islds.</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01</th>\n",
       "      <td>Vatican City</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-01</th>\n",
       "      <td>Vatican City</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-01</th>\n",
       "      <td>Vatican City</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-01</th>\n",
       "      <td>Vatican City</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-01</th>\n",
       "      <td>Vatican City</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01</th>\n",
       "      <td>Bonaire</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01</th>\n",
       "      <td>Bonaire</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>Bonaire</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01</th>\n",
       "      <td>Bonaire</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>Bonaire</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>Saint Barthélemy</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01</th>\n",
       "      <td>Saint Barthélemy</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>Saint Barthélemy</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>Saint Barthélemy</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01</th>\n",
       "      <td>Saint Barthélemy</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-01</th>\n",
       "      <td>Panama Canal Zone</td>\n",
       "      <td>3659.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-01-01</th>\n",
       "      <td>Panama Canal Zone</td>\n",
       "      <td>1040.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964-01-01</th>\n",
       "      <td>Panama Canal Zone</td>\n",
       "      <td>1508.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965-01-01</th>\n",
       "      <td>Panama Canal Zone</td>\n",
       "      <td>3818.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966-01-01</th>\n",
       "      <td>Panama Canal Zone</td>\n",
       "      <td>2530.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-01</th>\n",
       "      <td>Fmr Dem. Rep. of Vietnam</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-01-01</th>\n",
       "      <td>Fmr Dem. Rep. of Vietnam</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964-01-01</th>\n",
       "      <td>Fmr Dem. Rep. of Vietnam</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965-01-01</th>\n",
       "      <td>Fmr Dem. Rep. of Vietnam</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966-01-01</th>\n",
       "      <td>Fmr Dem. Rep. of Vietnam</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             location_name_short_en  export_value\n",
       "year                                                             \n",
       "1988-01-01                                    Aruba          0.00\n",
       "1989-01-01                                    Aruba          0.00\n",
       "1990-01-01                                    Aruba          0.00\n",
       "1991-01-01                                    Aruba          0.00\n",
       "1992-01-01                                    Aruba          0.00\n",
       "1988-01-01                              Afghanistan          0.00\n",
       "1989-01-01                              Afghanistan          0.00\n",
       "1990-01-01                              Afghanistan          0.00\n",
       "1991-01-01                              Afghanistan          0.00\n",
       "1992-01-01                              Afghanistan          0.00\n",
       "1988-01-01                                   Angola          0.00\n",
       "1989-01-01                                   Angola       1317.00\n",
       "1990-01-01                                   Angola       1597.00\n",
       "1991-01-01                                   Angola       5947.00\n",
       "1992-01-01                                   Angola          0.00\n",
       "1988-01-01                                 Anguilla          0.00\n",
       "1989-01-01                                 Anguilla        572.00\n",
       "1990-01-01                                 Anguilla          0.00\n",
       "1991-01-01                                 Anguilla      11258.00\n",
       "1992-01-01                                 Anguilla          0.00\n",
       "1988-01-01                                  Albania          0.00\n",
       "1989-01-01                                  Albania          0.00\n",
       "1990-01-01                                  Albania          0.00\n",
       "1991-01-01                                  Albania          0.00\n",
       "1992-01-01                                  Albania       1088.00\n",
       "1988-01-01                                  Andorra       6220.00\n",
       "1989-01-01                                  Andorra       3206.00\n",
       "1990-01-01                                  Andorra       6607.00\n",
       "1991-01-01                                  Andorra       2894.00\n",
       "1992-01-01                                  Andorra      17287.00\n",
       "...                                             ...           ...\n",
       "2000-01-01  South Georgia and South Sandwich Islds.          0.00\n",
       "2001-01-01  South Georgia and South Sandwich Islds.          0.00\n",
       "2002-01-01  South Georgia and South Sandwich Islds.          0.00\n",
       "2003-01-01  South Georgia and South Sandwich Islds.          0.00\n",
       "2004-01-01  South Georgia and South Sandwich Islds.          0.00\n",
       "2000-01-01                             Vatican City          0.00\n",
       "2001-01-01                             Vatican City          0.00\n",
       "2002-01-01                             Vatican City          0.00\n",
       "2003-01-01                             Vatican City          0.00\n",
       "2004-01-01                             Vatican City          0.00\n",
       "2011-01-01                                  Bonaire          0.00\n",
       "2012-01-01                                  Bonaire          0.00\n",
       "2013-01-01                                  Bonaire          0.00\n",
       "2014-01-01                                  Bonaire          0.00\n",
       "2015-01-01                                  Bonaire          0.00\n",
       "2013-01-01                         Saint Barthélemy          0.00\n",
       "2014-01-01                         Saint Barthélemy          0.00\n",
       "2015-01-01                         Saint Barthélemy          0.00\n",
       "2016-01-01                         Saint Barthélemy          0.00\n",
       "2017-01-01                         Saint Barthélemy          0.00\n",
       "1962-01-01                        Panama Canal Zone       3659.50\n",
       "1963-01-01                        Panama Canal Zone       1040.00\n",
       "1964-01-01                        Panama Canal Zone       1508.25\n",
       "1965-01-01                        Panama Canal Zone       3818.50\n",
       "1966-01-01                        Panama Canal Zone       2530.25\n",
       "1962-01-01                 Fmr Dem. Rep. of Vietnam          0.00\n",
       "1963-01-01                 Fmr Dem. Rep. of Vietnam          0.00\n",
       "1964-01-01                 Fmr Dem. Rep. of Vietnam          0.00\n",
       "1965-01-01                 Fmr Dem. Rep. of Vietnam          0.00\n",
       "1966-01-01                 Fmr Dem. Rep. of Vietnam          0.00\n",
       "\n",
       "[1250 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_df = y.groupby(['location_name_short_en'])\n",
    "country_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15, 10))\n",
    "for i, j in enumerate(y):\n",
    "    fig.add_subplot(9, 3, i+1)\n",
    "    y.loc[:, [str(j), str(i)]].plot(ax=axes.flat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reviewing plots of the density of observations can provide further insight into the structure of the data:__\n",
    "- The distribution is not perfectly Gaussian (normal distribution).\n",
    "- The distribution is left shifted.\n",
    "- Transformations might be useful prior to modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "from matplotlib import pyplot\n",
    "pyplot.figure(1)\n",
    "pyplot.subplot(211)\n",
    "y.location_name_short_en.hist()\n",
    "pyplot.subplot(212)\n",
    "y.location_name_short_en.plot(kind='kde')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Box and Whisker Plots:__\n",
    "- Median values across years confirms an upwards trend\n",
    "- Steady increase in the spread, or middle 50% of the data (boxes) over time\n",
    "- A model considering seasonality might work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "sns.boxplot(y.passengers.index.year, y.passengers, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Decomposing using statsmodel:__\n",
    "- We can use statsmodels to perform a decomposition of this time series. \n",
    "- The decomposition of time series is a statistical task that deconstructs a time series into several components, each representing one of the underlying categories of patterns. \n",
    "- With statsmodels we will be able to see the trend, seasonal, and residual components of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 18, 8\n",
    "decomposition = sm.tsa.seasonal_decompose(y, model='multiplicative')\n",
    "fig = decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity\n",
    "- A Time Series is said to be stationary if its statistical properties such as mean, variance remain constant over time.\n",
    "- Most of the Time Series models work on the assumption that the TS is stationary. Major reason for this is that there are many ways in which a series can be non-stationary, but only one way for stationarity.\n",
    "- Intuitively, we can say that if a Time Series has a particular behaviour over time, there is a very high probability that it will follow the same in the future. \n",
    "- Also, the theories related to stationary series are more mature and easier to implement as compared to non-stationary series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We can check stationarity using the following:__\n",
    "\n",
    "- __ACF and PACF plots__: If the time series is stationary, the ACF/PACF plots will show a __quick drop-off in correlation__ after a small amount of lag between points.\n",
    "- __Plotting Rolling Statistics__: We can plot the moving average or moving variance and see if it varies with time. Moving average/variance is for any instant ‘t’, the average/variance of the last year, i.e. last 12 months.\n",
    "- __Augmented Dickey-Fuller Test:__ This is one of the statistical tests for checking stationarity. Here the null hypothesis is that the TS is non-stationary. The test results comprise of a Test Statistic and some Critical Values for difference confidence levels. If the ‘Test Statistic’ is less than the ‘Critical Value’, we can reject the null hypothesis and say that the series is stationary. Refer this article for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACF and PACF plots\n",
    "\n",
    "- Let's review the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots\n",
    "- If the time series is stationary, the ACF/PACF plots will show a __quick drop-off in correlation__ after a small amount of lag between points.\n",
    "- This data is non-stationary as a high number of previous observations are correlated with future values.\n",
    "- Confidence intervals are drawn as a cone. \n",
    "- By default, this is set to a 95% confidence interval, suggesting that correlation values outside of this code are very likely a correlation and not a statistical fluke.\n",
    "- The partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "pyplot.figure()\n",
    "pyplot.subplot(211)\n",
    "plot_acf(y.passengers, ax=pyplot.gca(), lags = 30)\n",
    "pyplot.subplot(212)\n",
    "plot_pacf(y.passengers, ax=pyplot.gca(), lags = 30)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Rolling Statistics\n",
    "- We observe that the rolling mean and Standard deviation are not constant with respect to time (increasing trend)\n",
    "- The time series is hence not stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determing rolling statistics\n",
    "rolmean = y.rolling(2).mean()\n",
    "rolstd = y.rolling(12).std()\n",
    "\n",
    "#Plot rolling statistics:\n",
    "orig = plt.plot(y, color='blue',label='Original')\n",
    "mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Rolling Mean & Standard Deviation')\n",
    "plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Dickey-Fuller Test\n",
    "- The intuition behind the test is that if the series is integrated then the lagged level of the series y(t-1)  will provide no relevant information in predicting the change in y(t).\n",
    "- Null hypothesis: The time series is not stationary\n",
    "- Rejecting the null hypothesis (i.e. a very low p-value) will indicate staionarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Dickey-Fuller test:\n",
    "print ('Results of Dickey-Fuller Test:')\n",
    "dftest = adfuller(y.passengers, autolag='AIC')\n",
    "dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "for key,value in dftest[4].items():\n",
    "    dfoutput['Critical Value (%s)'%key] = value\n",
    "print (dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(timeseries):\n",
    "    \n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(12).mean()\n",
    "    rolstd = timeseries.rolling(12).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print ('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Time Series Stationary\n",
    "There are 2 major reasons behind non-stationaruty of a TS:\n",
    "\n",
    "1. __Trend__ – varying mean over time. For eg, in this case we saw that on average, the number of passengers was growing over time.\n",
    "2. __Seasonality__ – variations at specific time-frames. eg people might have a tendency to buy cars in a particular month because of pay increment or festivals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "- We can apply transformation which penalize higher values more than smaller values. These can be taking a log, square root, cube root, etc. Lets take a log transform here for simplicity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Scale Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ts_log = np.log(y)\n",
    "plt.plot(ts_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other possible transformations:\n",
    "- Exponential tranformation\n",
    "- Box Cox transformation\n",
    "- Square root transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques to remove Trend - Smoothing\n",
    "- Smoothing is taking rolling averages over windows of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving Average\n",
    "- We take average of ‘k’ consecutive values depending on the frequency of time series. \n",
    "- Here we can take the average over the past 1 year, i.e. last 12 values. \n",
    "- A drawback in this particular approach is that the time-period has to be strictly defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg = ts_log.rolling(12).mean()\n",
    "plt.plot(ts_log)\n",
    "plt.plot(moving_avg, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_moving_avg_diff = ts_log.passengers - moving_avg.passengers\n",
    "ts_log_moving_avg_diff.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ts_log_moving_avg_diff.dropna(inplace=True)\n",
    "test_stationarity(ts_log_moving_avg_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponentially weighted moving average:\n",
    "- To overcome the problem of choosing a defined window in moving average, we can use exponential weighted moving average\n",
    "- We take a ‘weighted moving average’ where more recent values are given a higher weight. \n",
    "- There can be many technique for assigning weights. A popular one is exponentially weighted moving average where weights are assigned to all the previous values with a decay factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expwighted_avg = ts_log.ewm(com=0.5).mean()\n",
    "#df.ewm(com=0.5).mean()\n",
    "plt.plot(ts_log)\n",
    "plt.plot(expwighted_avg, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ts_log_ewma_diff = ts_log.passengers - expwighted_avg.passengers\n",
    "test_stationarity(ts_log_ewma_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Techniques to remove Seasonality and Trend\n",
    "- The simple trend reduction techniques discussed before don’t work in all cases, particularly the ones with high seasonality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differencing\n",
    "- In this technique, we take the difference of the observation at a particular instant with that at the previous instant. \n",
    "- First order differencing in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_diff = ts_log.passengers - ts_log.passengers.shift()\n",
    "plt.plot(ts_log_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_diff.dropna(inplace=True)\n",
    "test_stationarity(ts_log_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decomposition\n",
    "- In this approach, both trend and seasonality are modeled separately and the remaining part of the series is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(ts_log)\n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.plot(ts_log, label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_decompose = residual.passengers\n",
    "ts_log_decompose.dropna(inplace=True)\n",
    "test_stationarity(ts_log_decompose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series forecasting\n",
    "[Statsmodel example notebooks](https://github.com/statsmodels/statsmodels/tree/master/examples/notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregression (AR)\n",
    "- The autoregression (AR) method models the next step in the sequence as a linear function of the observations at prior time steps.\n",
    "- __Number of AR (Auto-Regressive) terms (p):__ p is the parameter associated with the auto-regressive aspect of the model, which incorporates past values i.e lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AR\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model = AR(ts_log_diff)\n",
    "model_fit = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts_log_diff)\n",
    "plt.plot(model_fit.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'% np.nansum((model_fit.fittedvalues-ts_log_diff)**2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversing the transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fitted or predicted values:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff = pd.Series(model_fit.fittedvalues, copy=True)\n",
    "print (predictions_ARIMA_diff.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cumulative Sum to reverse differencing:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n",
    "print (predictions_ARIMA_diff_cumsum.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Adding 1st month value which was previously removed while differencing:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_log = pd.Series(ts_log.passengers.iloc[0], index=ts_log.index)\n",
    "predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\n",
    "predictions_ARIMA_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Taking Exponent to reverse Log Transform:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA = np.exp(predictions_ARIMA_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(y.passengers)\n",
    "plt.plot(predictions_ARIMA)\n",
    "plt.title('RMSE: %.4f'% np.sqrt(np.nansum((predictions_ARIMA-y.passengers)**2)/len(y.passengers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast quality scoring metrics\n",
    "- __R squared__\n",
    "- __Mean Absolute Error__\n",
    "- __Median Absolute Error__\n",
    "- __Mean Squared Error__\n",
    "- __Mean Squared Logarithmic Error__\n",
    "- __Mean Absolute Percentage Error__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error, mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__R squared__, coefficient of determination (it can be interpreted as a percentage of variance explained by the model), (-inf, 1] \n",
    "- sklearn.metrics.r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y.passengers, predictions_ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mean Absolute Error__, it is an interpretable metric because it has the same unit of measurement as the initial series, [0, +inf)\n",
    "- sklearn.metrics.mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean_absolute_error(y.passengers, predictions_ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Median Absolute Error__, again an interpretable metric, particularly interesting because it is robust to outliers, [0, +inf)\n",
    "- sklearn.metrics.median_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "median_absolute_error(y.passengers, predictions_ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mean Squared Error__, most commonly used, gives higher penalty to big mistakes and vise versa, [0, +inf)\n",
    "- sklearn.metrics.mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y.passengers, predictions_ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mean Squared Logarithmic Error__, practically the same as MSE but we initially take logarithm of the series, as a result we give attention to small mistakes as well, usually is used when data has exponential trends, [0, +inf)\n",
    "- sklearn.metrics.mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_log_error(y.passengers, predictions_ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mean Absolute Percentage Error__, same as MAE but percentage, — very convenient when you want to explain the quality of the model to your management, [0, +inf), \n",
    "- not implemented in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y.passengers, predictions_ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Function to evaluate forecast using above metrics:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_forecast(y,pred):\n",
    "    results = pd.DataFrame({'r2_score':r2_score(y, pred),\n",
    "                           }, index=[0])\n",
    "    results['mean_absolute_error'] = mean_absolute_error(y, pred)\n",
    "    results['median_absolute_error'] = median_absolute_error(y, pred)\n",
    "    results['mse'] = mean_squared_error(y, pred)\n",
    "    results['msle'] = mean_squared_log_error(y, pred)\n",
    "    results['mape'] = mean_absolute_percentage_error(y, pred)\n",
    "    results['rmse'] = np.sqrt(results['mse'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_forecast(y.passengers, predictions_ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RMSE has the benefit of penalizing large errors more so can be more appropriate in some cases, for example, if being off by 10 is more than twice as bad as being off by 5. But if being off by 10 is just twice as bad as being off by 5, then MAE is more appropriate.\n",
    "\n",
    "- From an interpretation standpoint, MAE is clearly the winner. RMSE does not describe average error alone and has other implications that are more difficult to tease out and understand.\n",
    "\n",
    "- On the other hand, one distinct advantage of RMSE over MAE is that RMSE avoids the use of taking the absolute value, which is undesirable in many mathematical calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Average (MA)\n",
    "\n",
    "- __Number of MA (Moving Average) terms (q):__ q is size of the moving average part window of the model i.e. lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where e(i) is the difference between the moving average at ith instant and actual value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MA example\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "from random import random\n",
    "\n",
    "# fit model\n",
    "model = ARMA(ts_log_diff, order=(0, 1))\n",
    "model_fit = model.fit(disp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts_log_diff)\n",
    "plt.plot(model_fit.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'% np.nansum((model_fit.fittedvalues-ts_log_diff)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Moving Average (ARMA)\n",
    "\n",
    "- __Number of AR (Auto-Regressive) terms (p):__ p is the parameter associated with the auto-regressive aspect of the model, which incorporates past values i.e lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5).\n",
    "- __Number of MA (Moving Average) terms (q):__ q is size of the moving average part window of the model i.e. lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where e(i) is the difference between the moving average at ith instant and actual value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARMA example\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "from random import random\n",
    "\n",
    "# fit model\n",
    "model = ARMA(ts_log_diff, order=(2, 1))\n",
    "model_fit = model.fit(disp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts_log_diff)\n",
    "plt.plot(model_fit.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'% np.nansum((model_fit.fittedvalues-ts_log_diff)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Integrated Moving Average (ARIMA)\n",
    "In an ARIMA model there are 3 parameters that are used to help model the major aspects of a times series: seasonality, trend, and noise. These parameters are labeled p,d,and q.\n",
    "\n",
    "- __Number of AR (Auto-Regressive) terms (p):__ p is the parameter associated with the auto-regressive aspect of the model, which incorporates past values i.e lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5).\n",
    "- __Number of Differences (d):__ d is the parameter associated with the integrated part of the model, which effects the amount of differencing to apply to a time series.\n",
    "- __Number of MA (Moving Average) terms (q):__ q is size of the moving average part window of the model i.e. lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where e(i) is the difference between the moving average at ith instant and actual value. \n",
    "\n",
    "<br>__Observations from EDA on the time series:__\n",
    "- Non stationarity implies at least one level of differencing (d) is required in ARIMA\n",
    "- [The next step is to select the lag values for the Autoregression (AR) and Moving Average (MA) parameters, p and q respectively, using PACF, ACF plots](https://people.duke.edu/~rnau/411arim3.htm)\n",
    "\n",
    "[Tuning ARIMA parameters](https://machinelearningmastery.com/tune-arima-parameters-python/)\n",
    "\n",
    "\n",
    "Note: A problem with ARIMA is that it does not support seasonal data. That is a time series with a repeating cycle. ARIMA expects data that is either not seasonal or has the seasonal component removed, e.g. seasonally adjusted via methods such as seasonal differencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = y.passengers - y.passengers.shift()\n",
    "ts.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ACF and PACF plots after differencing:__\n",
    "- Confidence intervals are drawn as a cone. \n",
    "- By default, this is set to a 95% confidence interval, suggesting that correlation values outside of this code are very likely a correlation and not a statistical fluke.\n",
    "- AR(1) process -- has ACF tailing out and PACF cutting off at lag=1\n",
    "- AR(2) process -- has ACF tailing out and PACF cutting off at lag=2\n",
    "- MA(1) process -- has ACF cut off at lag=1\n",
    "- MA(2) process -- has ACF cut off at lag=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.subplot(211)\n",
    "plot_acf(ts, ax=pyplot.gca(),lags=30)\n",
    "pyplot.subplot(212)\n",
    "plot_pacf(ts, ax=pyplot.gca(),lags=30)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting ACF plots\n",
    "\n",
    "\n",
    "ACF Shape\t| Indicated Model |\n",
    "-- | -- |\n",
    "Exponential, decaying to zero |\tAutoregressive model. Use the partial autocorrelation plot to identify the order of the autoregressive model |\n",
    "Alternating positive and negative, decaying to zero\tAutoregressive model. |  Use the partial autocorrelation plot to help identify the order. |\n",
    "One or more spikes, rest are essentially zero | Moving average model, order identified by where plot becomes zero. |\n",
    "Decay, starting after a few lags |\tMixed autoregressive and moving average (ARMA) model. | \n",
    "All zero or close to zero | Data are essentially random. |\n",
    "High values at fixed intervals | Include seasonal autoregressive term. |\n",
    "No decay to zero |\tSeries is not stationary |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide into train and validation set\n",
    "train = y[:int(0.75*(len(y)))]\n",
    "valid = y[int(0.75*(len(y))):]\n",
    "\n",
    "#plotting the data\n",
    "train['passengers'].plot()\n",
    "valid['passengers'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ARIMA example\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# fit model\n",
    "model = ARIMA(train, order=(1, 1, 1))\n",
    "model_fit = model.fit(disp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = valid.index.min()\n",
    "end_index = valid.index.max()\n",
    "\n",
    "#Predictions\n",
    "predictions = model_fit.predict(start=start_index, end=end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report performance\n",
    "mse = mean_squared_error(y[start_index:end_index], predictions)\n",
    "rmse = sqrt(mse)\n",
    "print('RMSE: {}, MSE:{}'.format(rmse,mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y.passengers)\n",
    "plt.plot(predictions, color='red')\n",
    "plt.title('RMSE: %.4f'% rmse)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fitted or predicted values:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff = pd.Series(predictions, copy=True)\n",
    "print (predictions_ARIMA_diff.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cumulative Sum to reverse differencing:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n",
    "print (predictions_ARIMA_diff_cumsum.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Adding 1st month value which was previously removed while differencing:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_log = pd.Series(valid.passengers.iloc[0], index=valid.index)\n",
    "predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\n",
    "predictions_ARIMA_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Taking Exponent to reverse Log Transform:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(y.passengers)\n",
    "plt.plot(predictions_ARIMA_log)\n",
    "plt.title('RMSE: %.4f'% np.sqrt(np.nansum((predictions_ARIMA_log-ts)**2)/len(ts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_forecast(y[start_index:end_index], predictions_ARIMA_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#building the model\n",
    "from pyramid.arima import auto_arima\n",
    "model = auto_arima(train, trace=True, error_action='ignore', suppress_warnings=True)\n",
    "model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "forecast = model.predict(n_periods=len(valid))\n",
    "forecast = pd.DataFrame(forecast,index = valid.index,columns=['Prediction'])\n",
    "\n",
    "#plot the predictions for validation set\n",
    "plt.plot(y.passengers, label='Train')\n",
    "#plt.plot(valid, label='Valid')\n",
    "plt.plot(forecast, label='Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_forecast(valid, forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal Autoregressive Integrated Moving-Average (SARIMA)\n",
    "Seasonal Autoregressive Integrated Moving Average, SARIMA or Seasonal ARIMA, is an extension of ARIMA that explicitly supports univariate time series data with a seasonal component.\n",
    "\n",
    "It adds three new hyperparameters to specify the autoregression (AR), differencing (I) and moving average (MA) for the seasonal component of the series, as well as an additional parameter for the period of the seasonality.\n",
    "\n",
    "__Trend Elements:__\n",
    "\n",
    "There are three trend elements that require configuration. They are the same as the ARIMA model, specifically:\n",
    "\n",
    "- p: Trend autoregression order.\n",
    "- d: Trend difference order.\n",
    "- q: Trend moving average order.\n",
    "\n",
    "__Seasonal Elements:__\n",
    "\n",
    "There are four seasonal elements that are not part of ARIMA that must be configured; they are:\n",
    "\n",
    "- P: Seasonal autoregressive order.\n",
    "- D: Seasonal difference order.\n",
    "- Q: Seasonal moving average order.\n",
    "- m: The number of time steps for a single seasonal period. For example, an S of 12 for monthly data suggests a yearly seasonal cycle.\n",
    "\n",
    "__SARIMA notation:__\n",
    "SARIMA(p,d,q)(P,D,Q,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMA example\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# fit model\n",
    "model = SARIMAX(train, order=(3, 1, 3), seasonal_order=(1, 1, 1, 1))\n",
    "model_fit = model.fit(disp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = valid.index.min()\n",
    "end_index = valid.index.max()\n",
    "\n",
    "#Predictions\n",
    "predictions = model_fit.predict(start=start_index, end=end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report performance\n",
    "mse = mean_squared_error(y[start_index:end_index], predictions)\n",
    "rmse = sqrt(mse)\n",
    "print('RMSE: {}, MSE:{}'.format(rmse,mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y)\n",
    "plt.plot(predictions)\n",
    "plt.title('RMSE: %.4f'% rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_forecast(y[start_index:end_index], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto - SARIMA\n",
    "\n",
    "[auto_arima documentation for selecting best model](https://www.alkaline-ml.com/pmdarima/tips_and_tricks.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the model\n",
    "from pyramid.arima import auto_arima\n",
    "model = auto_arima(train, trace=True, error_action='ignore', suppress_warnings=True, seasonal=True, m=6, stepwise=True)\n",
    "model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = valid.index.min()\n",
    "end_index = valid.index.max()\n",
    "\n",
    "#Predictions\n",
    "pred = model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(n_periods=len(valid))\n",
    "pred = pd.DataFrame(pred,index = valid.index,columns=['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = model.predict(n_periods=len(valid))\n",
    "forecast = pd.DataFrame(forecast,index = valid.index,columns=['Prediction'])\n",
    "\n",
    "#plot the predictions for validation set\n",
    "plt.plot(y.passengers, label='Train')\n",
    "#plt.plot(valid, label='Valid')\n",
    "plt.plot(forecast, label='Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_forecast(y[start_index:end_index], forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = d = q = range(0, 2)\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 6) for x in list(itertools.product(p, d, q))]\n",
    "print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_aic = 999999999\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(train,\n",
    "                                            order=param,\n",
    "                                            seasonal_order=param_seasonal,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "            \n",
    "            results = mod.fit()\n",
    "            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "            \n",
    "            #Check for best model with lowest AIC\n",
    "            if results.aic < min_aic:\n",
    "                min_aic = results.aic\n",
    "                min_aic_model = results\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_aic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = valid.index.min()\n",
    "end_index = valid.index.max()\n",
    "\n",
    "#Predictions\n",
    "pred = min_aic_model.get_prediction(start=start_index,end=end_index, dynamic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_ci = pred.conf_int()\n",
    "ax = y['1949':].plot(label='observed')\n",
    "pred.predicted_mean.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7))\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Passengers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model diagnostics:__\n",
    "- Our primary concern is to ensure that the residuals of our model are uncorrelated and normally distributed with zero-mean. \n",
    "- If the seasonal ARIMA model does not satisfy these properties, it is a good indication that it can be further improved.\n",
    "\n",
    "The model diagnostic suggests that the model residual is normally distributed based on the following:\n",
    "\n",
    "- In the top right plot, the red KDE line follows closely with the N(0,1) line. Where, N(0,1) is the standard notation for a normal distribution with mean 0 and standard deviation of 1. This is a good indication that the residuals are normally distributed. \n",
    "- The qq-plot on the bottom left shows that the ordered distribution of residuals (blue dots) follows the linear trend of the samples taken from a standard normal distribution. Again, this is a strong indication that the residuals are normally distributed.\n",
    "- The residuals over time (top left plot) don't display any obvious seasonality and appear to be white noise. \n",
    "- This is confirmed by the autocorrelation (i.e. correlogram) plot on the bottom right, which shows that the time series residuals have low correlation with lagged versions of itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_diagnostics(figsize=(16, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_forecasted = pred.predicted_mean.values\n",
    "y_truth = y[start_index:end_index].passengers.values\n",
    "mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The Root Mean Squared Error of our forecasts is {}'.format(round(np.sqrt(mse), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_forecast(y_truth, y_forecasted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX\n",
    "- The implementation is called SARIMAX instead of SARIMA because the “X” addition to the method name means that the implementation also supports exogenous variables.\n",
    "- Exogenous variables are optional can be specified via the “exog” argument.\n",
    " - model = SARIMAX(data, exog=other_data, ...)\n",
    "- Examples of exogenous variables: Population, holidays, number of airline companies, major events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prophet\n",
    "- [Prophet](https://facebook.github.io/prophet/) is open source software released by Facebook's Core Data Science team.\n",
    "- Prophet is a procedure for forecasting time series data based on an additive/multiplicative model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. \n",
    "- It works best with time series that have strong seasonal effects and several seasons of historical data. \n",
    "- Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\n",
    "- The Prophet package provides intuitive parameters which are easy to tune. \n",
    "\n",
    "[Prophet example notebooks](https://github.com/facebook/prophet/tree/master/notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Trend parameters__\n",
    "\n",
    "- growth: 'linear' or 'logistic' to specify a linear or logistic trend\n",
    "- changepoints:\tList of dates at which to include potential changepoints (automatic if not specified)\n",
    "- n_changepoints: If changepoints in not supplied, you may provide the number of changepoints to be automatically included\n",
    "- changepoint_prior_scale: Parameter for changing flexibility of automatic changepoint selection\n",
    " \n",
    "\n",
    "__Seasonality and Holiday Parameters__\n",
    "\n",
    "- yearly_seasonality: Fit yearly seasonality\n",
    "- weekly_seasonality: Fit weekly seasonality\n",
    "- daily_seasonality: Fit daily seasonality\n",
    "- holidays: Feed dataframe containing holiday name and date\n",
    "- seasonality_prior_scale: Parameter for changing strength of seasonality model\n",
    "- holiday_prior_scale: Parameter for changing strength of holiday model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prophet requires the variable names in the time series to be:\n",
    "\n",
    "- y – Target\n",
    "- ds – Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prophet = pd.DataFrame()\n",
    "train_prophet['ds'] = train.index\n",
    "train_prophet['y'] = train.passengers.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prophet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "\n",
    "#instantiate Prophet with only yearly seasonality as our data is monthly \n",
    "model = Prophet( yearly_seasonality=True, seasonality_mode = 'multiplicative')\n",
    "model.fit(train_prophet) #fit the model with your dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for five months in the furure and MS - month start is the frequency\n",
    "future = model.make_future_dataframe(periods = 36, freq = 'MS') \n",
    "future.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now lets make the forecasts\n",
    "forecast = model.predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = model.plot(forecast)\n",
    "#plot the predictions for validation set\n",
    "\n",
    "plt.plot(valid, label='Valid', color = 'red', linewidth = 2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prophet = pd.DataFrame()\n",
    "y_prophet['ds'] = y.index\n",
    "y_prophet['y'] = y.passengers.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_prophet = y_prophet.set_index('ds')\n",
    "forecast_prophet = forecast.set_index('ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_forecast(y_prophet.y[start_index:end_index], forecast_prophet.yhat[start_index:end_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Time Series Forecast models\n",
    "\n",
    "1. Hyperparamter Optimization: Finding the optimal parameters of ARIMA/Prophet models.\n",
    "\n",
    "2. Exogenous variables (SARIMAX): Including external variables like campaigns, holidays, events, natural calamities etc.\n",
    "\n",
    "3. [Combining models for advanced time series predictions](https://www.kdnuggets.com/2016/11/combining-different-methods-create-advanced-time-series-prediction.html)\n",
    "\n",
    "4. [Long Short Term Memory Network (LSTM)](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "436px",
    "width": "256px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
