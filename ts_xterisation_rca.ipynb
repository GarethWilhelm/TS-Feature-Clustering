{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "436px",
        "width": "256px"
      },
      "number_sections": true,
      "sideBar": false,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "ts_xterisation_rca.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wgova/time_series_trade/blob/master/ts_xterisation_rca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2Gz5t-2O51R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ![Open In Colab][(https://colab.research.google.com/assets/colab-badge.svg)\n",
        "# (https://colab.research.google.com/github/wgova/time_series_trade/blob/master/ts_xterisation_rca.ipynb)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiKTMEoEOvHi",
        "colab_type": "text"
      },
      "source": [
        "<strong><b><font size=\"5\">Time Series Characterisation</font></b></strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": true,
        "id": "2K2P7h_wOvHl",
        "colab_type": "text"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Time-Series\" data-toc-modified-id=\"Time-Series-1\"><span class=\"toc-item-num\">\n",
        "1&nbsp;&nbsp;</span>Time Series</a></span></li><li><span><a href=\"#Importing-Libraries-for-time-series-forecasting\" data-toc-modified-id=\"Importing-Libraries-for-time-series-forecasting-3\"><span class=\"toc-item-num\">\n",
        "2&nbsp;&nbsp;</span>Importing Libraries for time series forecasting</a></span></li><li><span><a href=\"#Importing-data\" data-toc-modified-id=\"Importing-data-4\"><span class=\"toc-item-num\">\n",
        "3&nbsp;&nbsp;</span>Importing data</a></span></li><li><span><a href=\"#Data-Preprocessing-and-Visualization\" data-toc-modified-id=\"Data-Preprocessing-and-Visualization-5\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data Preprocessing and Visualization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stationarity\" data-toc-modified-id=\"Stationarity-4.1\"><span class=\"toc-item-num\">\n",
        "4.1&nbsp;&nbsp;</span>Stationarity</a></span><ul class=\"toc-item\"><li><span><a href=\"#ACF-and-PACF-plots\" data-toc-modified-id=\"ACF-and-PACF-plots-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>ACF and PACF plots</a></span></li><li><span><a href=\"#Plotting-Rolling-Statistics\" data-toc-modified-id=\"Plotting-Rolling-Statistics-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Plotting Rolling Statistics</a></span></li><li><span><a href=\"#Augmented-Dickey-Fuller-Test\" data-toc-modified-id=\"Augmented-Dickey-Fuller-Test-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Augmented Dickey-Fuller Test</a></span></li></ul></li><li><span><a href=\"#Making-Time-Series-Stationary\" data-toc-modified-id=\"Making-Time-Series-Stationary-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Making Time Series Stationary</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transformations\" data-toc-modified-id=\"Transformations-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Transformations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Log-Scale-Transformation\" data-toc-modified-id=\"Log-Scale-Transformation-4.2.1.1\"><span class=\"toc-item-num\">4.2.1.1&nbsp;&nbsp;</span>Log Scale Transformation</a></span></li><li><span><a href=\"#Other-possible-transformations:\" data-toc-modified-id=\"Other-possible-transformations:-4.2.1.2\"><span class=\"toc-item-num\">4.2.1.2&nbsp;&nbsp;</span>Other possible transformations:</a></span></li></ul></li><li><span><a href=\"#Techniques-to-remove-Trend---Smoothing\" data-toc-modified-id=\"Techniques-to-remove-Trend---Smoothing-5.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Techniques to remove Trend - Smoothing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Moving-Average\" data-toc-modified-id=\"Moving-Average-4.2.2.1\"><span class=\"toc-item-num\">4.2.2.1&nbsp;&nbsp;</span>Moving Average</a></span></li><li><span><a href=\"#Exponentially-weighted-moving-average:\" data-toc-modified-id=\"Exponentially-weighted-moving-average:-4.2.2.2\"><span class=\"toc-item-num\">4.2.2.2&nbsp;&nbsp;</span>Exponentially weighted moving average:</a></span></li></ul></li><li><span><a href=\"#Further-Techniques-to-remove-Seasonality-and-Trend\" data-toc-modified-id=\"Further-Techniques-to-remove-Seasonality-and-Trend-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Further Techniques to remove Seasonality and Trend</a></span><ul class=\"toc-item\"><li><span><a href=\"#Differencing\" data-toc-modified-id=\"Differencing-4.2.3.1\"><span class=\"toc-item-num\">4.2.3.1&nbsp;&nbsp;</span>Differencing</a></span></li><li><span><a href=\"#Decomposition\" data-toc-modified-id=\"Decomposition-4.2.3.2\"><span class=\"toc-item-num\">4.2.3.2&nbsp;&nbsp;</span>Decomposition</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHDDyFBVOvHn",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>DATA SOURCES</b>\n",
        "    \n",
        "This project makes use of is a time series of export data from __[The Observatory of Economic Complexity(OEC)](https://atlas.media.mit.edu/static/db/raw/year_origin_sitc_rev2.tsv.bz2)__ based on the  SITC (Standard International Trade Classification)** from 1962 - 2000, with data from __[The Center for International Data](http://cid.econ.ucdavis.edu/)__ from Robert Feenstra. The more recent data 2001 - 2017, is sourced from __[UN COMTRADE](http://comtrade.un.org/)__.\n",
        "\n",
        "Full **OEC** trade datasets are also available from a data dump on __[SITC Product 4 Digit](https://intl-atlas-downloads.s3.amazonaws.com/country_sitcproduct4digit_year.zip)__\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfYGmTcjOvHo",
        "colab_type": "text"
      },
      "source": [
        "# Importing Libraries for time series forecasting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkOipJgROvHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Run this for quick setup\n",
        "#!pip install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwZR4GHNOvH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "import statsmodels.robust\n",
        "import statsmodels.tsa.stattools as tsa\n",
        "\n",
        "from statsmodels import api as sm\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from statsmodels.graphics.tsaplots import plot_pacf\n",
        "from pylab import rcParams\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.colors\n",
        "matplotlib.rcParams['axes.labelsize'] = 14\n",
        "matplotlib.rcParams['xtick.labelsize'] = 12\n",
        "matplotlib.rcParams['ytick.labelsize'] = 12\n",
        "matplotlib.rcParams['text.color'] = 'k'\n",
        "import seaborn as sns\n",
        "\n",
        "from random import random\n",
        "\n",
        "import warnings; import os\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.style.available\n",
        "plt.style.use('bmh')\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyjFlXNaOvIA",
        "colab_type": "text"
      },
      "source": [
        "# Importing data\n",
        "- Dataset: Trade data:\n",
        "    - location_id: /- product_id: SITC 4 digit code system\n",
        "    - year: from 1963 to 2017  - export_value: / - import_value: - / - export_rca:\n",
        "    - is_new: / - cog: / - distance: / - normalized_distance: / - normalized_cog:\n",
        "    - normalized_pci: /- export_rpop: / - sitc_eci: / - sitc_coi: / - pci: /- location_code: /- location_name_short_en:\n",
        "    - sitc_product_code: / - sitc_product_name_short_en: /- Unit: arbitrary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g43f6EF-OvIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "product = 'engine_parts' # Create a dictionary to store product code and sitc code\n",
        "code = 7132\n",
        "experiment = 'rca_values'\n",
        "\n",
        "def load_data(csv_name,trade_form,target_value, code_system,sitc_code,ts_type):\n",
        "    '''\n",
        "    Ingests SITC multi-time series trade data in csv format, filters for one product code, removes zero values for \n",
        "    target value, screens target values and renames target columns & creates dataframe for the target columns\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    csv:\n",
        "    trade_form:\n",
        "    target_value:\n",
        "    code_system:\n",
        "    sitc_code:\n",
        "    ts_type:\n",
        "    '''\n",
        "    data = pd.read_csv(csv_name)#,header=None\n",
        "    # TO DO: Create for loop to read sitc code from dictionary\n",
        "    dframe = data.loc[data[code_system]==sitc_code]\n",
        "    dframe.rename(columns={'location_name_short_en': 'exporter','export_rca': 'rca'}, inplace=True)\n",
        "    dframe = dframe[[ts_type,trade_form,target_value]]\n",
        "    dframe = dframe[dframe[trade_form] != 'Undeclared Countries']\n",
        "    dframe[target_value].replace('',0, inplace=True)\n",
        "    dframe = dframe[dframe[target_value] != 0]\n",
        "    dframe = dframe[dframe[target_value] >= 0.09]\n",
        "    dframe = dframe[dframe[target_value] <= 20]\n",
        "    return dframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhQOIIvEOvIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "1ad3ed38-c46a-48ae-80b3-ee9d79a21f55"
      },
      "source": [
        "xdf = load_data('sitc4digit_year.csv','exporter','rca','sitc_product_code',code,'year')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-23cc45d96668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sitc4digit_year.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'exporter'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rca'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sitc_product_code'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-9697175fb5ac>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(csv_name, trade_form, target_value, code_system, sitc_code, ts_type)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mts_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     '''\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#,header=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# TO DO: Create for loop to read sitc code from dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcode_system\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0msitc_code\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File sitc4digit_year.csv does not exist: 'sitc4digit_year.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "NA-VfuCuOvIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exporters = xdf['exporter'].nunique()\n",
        "time = xdf['year'].nunique()\n",
        "unknown = xdf['exporter'].loc[xdf['exporter']=='Undeclared Countries'].count()\n",
        "ctotal = xdf.exporter.count()\n",
        "zerorca = xdf['rca'][xdf['rca']==0].count()\n",
        "nulls = xdf['rca'].isnull().sum().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsMsuTFlOvIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Shape of the dataframe: {xdf.shape}')\n",
        "print(f'The data covers {time} years from {xdf.year.min()} to {xdf.year.max()} for {exporters} countries')\n",
        "print(f'The rca values range from {xdf.rca.min()} to {xdf.rca.max()}')\n",
        "print(f'Countries not declared are {unknown} out of {ctotal}')\n",
        "print(f'Zero export values: {zerorca}')\n",
        "print(f'Null export values: {nulls}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avcA8W8AOvId",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing and Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pp_-UauOvIe",
        "colab_type": "text"
      },
      "source": [
        "__Converting to datetime format:__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knQL-cblOvIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xdf['year'] = pd.to_datetime(xdf['year'], format='%Y')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bGsSwsHdOvIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xdf.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH5vTx-vOvIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_images(ts_groups,analysis,exporter=None):\n",
        "    '''\n",
        "    analysis: can be from these options ['summary',box plots','decompose',acf_pacf','adf_test','rolling_stats']\n",
        "    '''\n",
        "    if exporter is not None:\n",
        "        country = ts_groups[exporter].name\n",
        "    else:\n",
        "        country = ''\n",
        "    try:\n",
        "        save_to = os.path.join(path,analysis,country)\n",
        "        print(f'Images stored in {save_to}')\n",
        "    except:\n",
        "        save_to = os.path.join(path,analysis)\n",
        "        print(f'Images stored in {save_to}')\n",
        "    return save_to"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txBeDnU4OvIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adapted from https://stackoverflow.com/questions/30942755/plotting-multiple-time-series-after-a-groupby-in-pandas\n",
        "# Modified from https://www.programcreek.com/python/example/98021/matplotlib.dates.YearLocator\n",
        "from matplotlib.dates import YearLocator, MonthLocator, DateFormatter\n",
        "\n",
        "def plot_multi_ts(dframe, ts_label, grp_label, values_label, product,experiment,figsize=(30,15),title=None):\n",
        "    '''\n",
        "    Plots multiple time series on one chart by first grouping time series based on series names from column in dataframe\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dframe : timeseries Pandas dataframe\n",
        "    ts_label : string\n",
        "        The name of the df column that has the datetime timestamp x-axis values.\n",
        "    grp_label : string\n",
        "        The column name in dframe for groupby method.\n",
        "    values_label : string\n",
        "        The column name in dframe for the y-axis.\n",
        "    figsize : tuple of two integers\n",
        "        Figure size of the resulting plot, e.g. (20, 7)\n",
        "    title : string\n",
        "        Optional title\n",
        "    '''\n",
        "    xtick_locator = YearLocator()\n",
        "    xtick_dateformatter = DateFormatter('%Y')\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    for key, grp in dframe.groupby([grp_label]):\n",
        "        ax = grp.plot(ax=ax, kind='line', x=ts_label, y=values_label, label=key, marker='o')\n",
        "    ax.xaxis.set_major_locator(xtick_locator)\n",
        "    ax.xaxis.set_major_formatter(xtick_dateformatter)\n",
        "    ax.autoscale_view()\n",
        "    ax.legend(ncol=5, loc='upper left')\n",
        "    plot = f'Line_plots_{product}_{code}.png'\n",
        "    #_ = plt.xticks(rotation=0, )\n",
        "    _ = plt.grid()\n",
        "    _ = plt.xlabel('year')\n",
        "    _ = plt.ylabel('Export rca value')\n",
        "    _ = plt.ylim(0, dframe[values_label].max() * 1.25)\n",
        "    if title is not None:\n",
        "        _ = plt.title(title)\n",
        "    analysis = 'summary'\n",
        "    file = save_images(dframe,analysis)\n",
        "    plt.savefig(file+plot)\n",
        "    _ = plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjyzcY0ZOvI1",
        "colab_type": "text"
      },
      "source": [
        "**Image storage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZMR4RooOvI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = os.path.join(os.getcwd(),f'images/{product}/{experiment}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4kD2E5vOvI7",
        "colab_type": "text"
      },
      "source": [
        "**Target folder to store individual images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRBjLB7ROvI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_multi_ts(xdf, 'year', 'exporter', 'rca', 'engine_parts',experiment,title=f'Global {experiment} trends for {product}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwqVl0lfOvJA",
        "colab_type": "text"
      },
      "source": [
        "A majority of countries have rca values below the mean value of *0.636*. A few countries have once-off or intermittently  high values preceeded by long periods with zero. These countries were excluded from the dataset and treated as outliers by limiting rca values to below 20, which was observed as the value most countries fall under. An example is Norfolk Islands shown below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNDXnJukOvJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rdf = xdf.set_index('year')\n",
        "#Norfolk = pd.DataFrame(rdf['rca'].loc[rdf['exporter']=='Norfolk Island'])\n",
        "#Norfolk.to_csv('norfolk.csv')\n",
        "norfolk = pd.read_csv('norfolk.csv')\n",
        "norfolk.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UyPjYkH4OvJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def topx(df,threshold):\n",
        "    grp = df.set_index('year')\n",
        "    grp = df.groupby([df.year.name, df.exporter.name]).mean().unstack()\n",
        "    grp.columns = grp.columns.droplevel()\n",
        "    thresh = int(len(grp) * threshold)\n",
        "    grp.dropna(thresh = thresh, axis = 1, inplace = True)\n",
        "    grp = grp.apply(pd.to_numeric, errors='coerce').fillna(0, downcast='infer')\n",
        "    return grp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1u3vlcNOvJL",
        "colab_type": "text"
      },
      "source": [
        "Some countries have changed names since 1962, and these names had to be updated along with the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mMh-iC4OvJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combine = ['Former USSR','Russian Federation','Fmr Dem. Yemen','Yemen']\n",
        "# for c in combine: \n",
        "df_grps = topx(xdf,0)\n",
        "df_grps.apply(pd.to_numeric, errors='coerce').fillna(0, downcast='infer')\n",
        "df_grps = df_grps.rename(columns={'Fmr. Pacific Islands': 'Pacific Islands'})\n",
        "df_grps['Russia'] = df_grps['Former USSR']+df_grps['Russian Federation']\n",
        "df_grps['Yemen'] = df_grps['Fmr Dem. Yemen']+df_grps['Yemen']\n",
        "df_grps = df_grps.drop(['Fmr Dem. Yemen','Norfolk Island','Former USSR','Russian Federation','Virgin Islands (U.S.)','Virgin Islands (British)'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOxqBJZ3OvJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y06N29ztOvJV",
        "colab_type": "text"
      },
      "source": [
        "__Setting index as the datetime column for easier manipulations:__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICTWPCwfOvJW",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "__Box and Whisker Plots will be used to check:__ \n",
        "\n",
        " - what time series have similar median values across different years\n",
        " - whether there is a steady increase in the spread, or middle 50% of the data (boxes) over time\n",
        " - best model if considering seasonality\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9WrQ80_MOvJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_groups(df, group):\n",
        "    fig1, ax1 = plt.subplots(figsize=(30,15))\n",
        "    #https://matplotlib.org/3.1.0/gallery/color/named_colors.html\n",
        "    colors = [\"gray\",\"brown\",\"darkviolet\",\"bisque\",\"b\",\"g\",\"r\",\"c\",\"gold\",\"darkolivegreen\",\"m\",\"teal\",\"rosybrown\",\"y\",\"orange\",\"k\",\"fuchsia\",\"lavender\",\"maroon\",\"lime\",\n",
        "              \"olive\",\"salmon\",\"peru\",\"aqua\"]\n",
        "    label=f'Global trends in {experiment} for {product}'\n",
        "    df.plot(ax=ax1,label=label,color=colors)\n",
        "    ax1.set_ylabel(f'Export {experiment}')\n",
        "    fig1.suptitle(f'Global exports for {product} using {experiment} for {group}')\n",
        "    plt.legend(ncol=8,loc='best')\n",
        "    name1 = f'Trend for {group}'\n",
        "    savefile = str(save_images(df,'histograms')+name1)\n",
        "    plt.savefig(savefile)\n",
        "    \n",
        "    fig2, ax2 = plt.subplots(figsize=(40,25))\n",
        "    fig2.suptitle(f'Box plots for {product} using {experiment} for {group}')\n",
        "    ax2 = sns.boxplot(x='exporter', y=\"value\", data=pd.melt(df))\n",
        "    name2 = f'Box_plots for {group}'\n",
        "    savefile = str(save_images(df,'box_plots')+name2)\n",
        "    plt.savefig(savefile)\n",
        "    plt.show()    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "oBeCYuGCOvJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_groups(df_grps, 'all_exporters')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJvlpSK1OvJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_exp = topx(xdf,0.6)\n",
        "plot_groups(avg_exp,'average_exporters')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SZDGmVbOvJi",
        "colab_type": "text"
      },
      "source": [
        "Setting a threshold for the percentage years in which no exports were reported based on empty data points for rca values eliminates outlier countries with erratic exports. This suggests that the engine parts might have been imported and possibly re-engineered for re-export in some of these years."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxELsQ5JOvJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Excluding countries with no reported exports for 60% of the time period makes exporters dropped from {len(df_grps.columns)} to {len(avg_exp.columns)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ayZU32Q_OvJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_exp = topx(xdf,0.85)\n",
        "plot_groups(best_exp,'best_exporters')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Cdx7AvQOvJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Excluding countries with no reported exports for 85% of the time period makes exporters dropped from {len(avg_exp.columns)} to {len(best_exp.columns)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3ZRqksVOvJu",
        "colab_type": "text"
      },
      "source": [
        "**Decomposition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk2pAl1cOvJv",
        "colab_type": "text"
      },
      "source": [
        "- statsmodels was used to perform a decomposition of the time series. \n",
        "- Decomposition deconstructs a time series into several components, each representing one of the underlying categories of patterns. \n",
        "- Statsmodels reveals the trend, seasonality, and residual components of our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjeyIYy1OvJw",
        "colab_type": "text"
      },
      "source": [
        "__Reviewing plots of the density of observations will provide further insight into the structure of the data:__\n",
        "- Is the distribution perfectly Gaussian (normal distribution)?\n",
        "- Skewness: which direction is the distribution?\n",
        "- If not Gaussian and skew, transformations will be necessary prior to modelling\n",
        "\n",
        "__Plot of histograms, kernel density estimation (KDE) and to show skewness in data for each country__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr47u1dfOvJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List of countries in the dataset\n",
        "exporters = avg_exp.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E4rQS_vOvJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for exporter in exporters:\n",
        "    print(exporters)\n",
        "    root = save_images(avg_exp,'decomposition')+exporter\n",
        "    plt.figure(figsize=(30,5)); \n",
        "    \n",
        "    plt.subplot(131)\n",
        "    plt.title(f'Histogram for {exporter}')\n",
        "    ax = avg_exp[exporter].plot(kind='hist')\n",
        "    ax1 = avg_exp[exporter].plot(kind='kde')\n",
        "    plt.ylabel('Count of rca values')\n",
        "    plt.xlim(1962,2016)\n",
        "    plt.ylim(0,80)\n",
        "    \n",
        "    plt.subplot(132)\n",
        "    plt.title(f'Density plot for {exporter}')\n",
        "    ax2 = sns.distplot(avg_exp[exporter],hist=True,rug=True)\n",
        "    plt.ylabel('Density')\n",
        "    plt.ylim(0,70)\n",
        "    \n",
        "    plt.subplot(131)\n",
        "    decomposition = sm.tsa.seasonal_decompose(avg_exp[exporter], model='additive')\n",
        "    ax3 = decomposition.plot()\n",
        "    plt.title(f'{avg_exp[exporter].name}_Time series decomposition',loc='center')\n",
        "    rcParams['figure.figsize'] = (30, 10)\n",
        "    \n",
        "    plt.savefig(root)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwYuV6VbOvJ5",
        "colab_type": "text"
      },
      "source": [
        "## Stationarity\n",
        "- Most of the Time Series models work on the assumption that the TS is stationary. \n",
        "- Major reason for this is that there are many ways in which a series can be non-stationary, but only one way for stationarity.\n",
        "- Intuitively, we can say that if a Time Series has a particular behaviour over time, there is a very high probability that it will follow the same behaviour in the future. \n",
        "- Also, the theories related to stationary series are more mature and easier to implement as compared to non-stationary series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iao9Knf3OvJ6",
        "colab_type": "text"
      },
      "source": [
        "__Notes from__ [Alkaline ML](https://www.alkaline-ml.com/pmdarima/tips_and_tricks.html)\n",
        "\n",
        "- tests of stationarity for testing a null hypothesis that an observable univariate time series is stationary around a deterministic trend (i.e. trend-stationary).\n",
        "- A time series is __stationary when its mean, variance and auto-correlation, etc., are constant over time__. \n",
        "- Many time-series methods may perform better when a time-series is stationary, since forecasting values becomes a far easier task for a stationary time series (high probability of behaving the same way)\n",
        "- ARIMAs that include differencing (i.e., d > 0) assume that the data becomes stationary after differencing. \n",
        "- This is called __difference-stationary__\n",
        "- Auto-correlation plots are an easy way to determine whether your time series is sufficiently stationary for modeling. \n",
        "- If the plot does not appear relatively stationary, your model will likely need a differencing term. \n",
        "- These can be determined by using an Augmented Dickey-Fuller test, or various other statistical testing methods. \n",
        "- Note that auto_arima will automatically determine the appropriate differencing term for you by default."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pkd4n3I1OvJ6",
        "colab_type": "text"
      },
      "source": [
        "In order to quantitatively determine whether we need to difference our data in order to make it stationary, we can conduct a test of stationarity\n",
        "\n",
        "__We can check stationarity using the following:__\n",
        "\n",
        "1. __ACF and PACF plots__:\n",
        "    - If the time series is stationary, the ACF/PACF plots will show a __quick drop-off in correlation__ after a small amount of lag between points.\n",
        "2. __Plotting Rolling Statistics__: \n",
        "    - We can plot the moving average or moving variance and see if it varies with time. \n",
        "    - __Moving average/variance__ is for any instant ‘t’, the average/variance of the last period (e.g year or last 12 months)\n",
        "3. __Augmented Dickey-Fuller Test [*]:__\n",
        "    - This is one of the statistical tests for checking stationarity.\n",
        "    - Here the __null hypothesis is that the TS is non-stationary__.\n",
        "     - The null hypothesis of the Augmented Dickey-Fuller is that there is a unit root, with the alternative that there is no unit root. \n",
        "     - If the pvalue is above a critical size, then we cannot reject that there is a unit root.\n",
        "     - The p-values are obtained through regression surface approximation from MacKinnon 1994, but using the updated 2010 tables\n",
        "     - If the p-value is close to significant, then the critical values should be used to judge whether to reject the null.\n",
        "     - The autolag option and maxlag for it are described in Greene\n",
        "    - The test results comprise of a Test Statistic and some Critical Values for difference confidence levels. \n",
        "    - If the ‘Test Statistic’ is less than the ‘Critical Value’, we can reject the null hypothesis and say that the series is stationary. \n",
        "    \n",
        "**References**:\n",
        "\n",
        "   1. W. Green.  \"Econometric Analysis,\" 5th ed., Pearson, 2003.\n",
        "    \n",
        "   2. Hamilton, J.D.  \"Time Series Analysis\".  Princeton, 1994.\n",
        "    \n",
        "   3. MacKinnon, J.G. 1994.  \"Approximate asymptotic distribution functions for\n",
        "    unit-root and cointegration tests.  `Journal of Business and Economic Statistics` 12, 167-76.\n",
        "    \n",
        "   4. MacKinnon, J.G. 2010. \"Critical Values for Cointegration Tests.\"  Queen's University, Dept of Economics, Working Papers.  Available at http://ideas.repec.org/p/qed/wpaper/1227.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwHBK0g2OvJ7",
        "colab_type": "text"
      },
      "source": [
        "### Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots\n",
        "\n",
        "- If the time series is stationary, the ACF/PACF plots will show a __quick drop-off in correlation__ after a small amount of lag between points.\n",
        "- Most of this data is stationary as a high number of previous observations are not correlated with future values.\n",
        "- Confidence intervals are drawn as a cone. \n",
        "- By default, this is set to a 95% confidence interval, suggesting that correlation values outside of this cone are very likely a correlation and not a statistical fluke.\n",
        "- The partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SqYkTe0OvJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for exporter in exporters:\n",
        "    savefile = save_images(avg_exp,'acf_pacf')\n",
        "    \n",
        "    plt.figure(figsize=(15,6))\n",
        "    plt.subplot(121)\n",
        "    plot_acf(avg_exp[exporter], ax=plt.gca(),title=f\"{avg_exp[exporter].name} - Autocorrelation\")\n",
        "    \n",
        "    plt.subplot(122); \n",
        "    plot_pacf(avg_exp[exporter], ax=plt.gca(),title=f\"{avg_exp[exporter].name} - Partial Autocorrelation\")\n",
        "    \n",
        "    plt.savefig(f'{savefile}_ACF_PACF')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eO9hXfpOvJ_",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "# TODO\n",
        "## Interpreting ACF plots\n",
        "\n",
        "\n",
        "ACF Shape\t| Indicated Model | Countries fitting description\n",
        "-- | -- |--\n",
        "Exponential, decaying to zero |\tAutoregressive model. Use the partial autocorrelation plot to identify the order of the autoregressive model |\n",
        "Alternating positive and negative, decaying to zero\tAutoregressive model. |  Use the partial autocorrelation plot to help identify the order. |\n",
        "One or more spikes, rest are essentially zero | Moving average model, order identified by where plot becomes zero. |\n",
        "Decay, starting after a few lags |\tMixed autoregressive and moving average (ARMA) model. | \n",
        "All zero or close to zero | Data are essentially random. |\n",
        "High values at fixed intervals | Include seasonal autoregressive term. |\n",
        "No decay to zero |\tSeries is not stationary |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA4KPZ-fOvKA",
        "colab_type": "text"
      },
      "source": [
        "### Plotting Rolling Statistics\n",
        "- The rolling mean and standard deviation are not constant with respect to time (increasing trend)\n",
        "- The time series is hence not stationary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpEsXI81OvKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for exporter in exporters:\n",
        "    savefile = save_images(avg_exp,'rolling_stats')\n",
        "    cycles = int((len(avg_exp[exporter]))/5)\n",
        "    plt.figure(figsize=(15,6))\n",
        "\n",
        "    rolmean = avg_exp[exporter].rolling(2).mean()\n",
        "    rolstd = avg_exp[exporter].rolling(cycles).std()\n",
        "        \n",
        "    #Plot rolling statistics:\n",
        "    orig = plt.plot(avg_exp[exporter], color='blue',label='Original')\n",
        "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
        "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
        "    \n",
        "    plt.legend(loc='best')\n",
        "    plt.title('{}: Rolling Mean & Standard Deviation'.format(avg_exp[exporter].name))\n",
        "    plt.savefig(str('{}_Rolling_Stats'.format(savefile)))\n",
        "    plt.show(block=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cA-yBTGOvKF",
        "colab_type": "text"
      },
      "source": [
        "### Augmented Dickey-Fuller  (ADF) Test\n",
        "- The intuition behind the test is that if the series is integrated then the lagged level of the series y(t-1)  will provide no relevant information in predicting the change in y(t).\n",
        "\n",
        "**Null hypothesis (Ho)**: The time series is not stationary, it presents heterocedasticity. In other words, the time series depends on itself (i.e.: yt depends on yt-1, yt-1 depends on yt-2\n",
        "- Rejecting the null hypothesis (i.e. a very low p-value) will indicate stationarity. Put simply, the t-test result is less than all critical values (1%,5%,10%)\n",
        "\n",
        "**Alternative hypothesis(Ha)**: the series is stationary\n",
        "\n",
        "__Reference__\n",
        "[ADF results description](https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.adfuller.html#statsmodels.tsa.stattools.adfuller)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftYT_NOaOvKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.tsa.stattools import adfuller, kpss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhxCKxBoOvKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ADF_Stationarity_Test(ts_groups):\n",
        "    \n",
        "    #countries = list(ts_groups.columns)\n",
        "    adf_results = {}\n",
        "    col_names=['Test Statistic','p_value','Lags used','Observations Used','1% Critical value','5% Critical value','10% Critical value','IC_Best']\n",
        "    data = pd.DataFrame(columns=col_names,index=exporters)\n",
        "\n",
        "    for exporter in exporters:\n",
        "        #Dickey-Fuller test\n",
        "        adf_results[exporter] = tsa.adfuller(ts_groups[exporter],autolag='AIC')\n",
        "        \n",
        "    for exporter in adf_results:\n",
        "        data.loc[exporter] = pd.Series({'Test Statistic':adf_results[exporter][0],\n",
        "                                       'p_value':adf_results[exporter][1],\n",
        "                                       'Lags used':adf_results[exporter][2],\n",
        "                                       'Observations Used':adf_results[exporter][3],\n",
        "                                       '1% Critical value': adf_results[exporter][4]['1%'],\n",
        "                                       '5% Critical value': adf_results[exporter][4]['5%'],\n",
        "                                       '10% Critical value': adf_results[exporter][4]['10%'],\n",
        "                                       'IC_Best': adf_results[exporter][5]\n",
        "                                      })\n",
        "    return data.transpose()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "uX1weljTOvKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adf_test = ADF_Stationarity_Test(avg_exp)\n",
        "adf_countries = adf_test.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYWeNc2LOvKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for exporter in adf_test:\n",
        "    savefile = save_images(adf_test,'adf_test')\n",
        "    adf_test[exporter].plot.bar()\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('{} - ADF Results'.format(adf_test[exporter].name))\n",
        "    plt.savefig(str('{}_ADF.png'.format(savefile)))\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG1yoXALOvKT",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "## TO DO   \n",
        "\n",
        "Automate p-value diagnosis: [Try this to automate p-value rejection for non-stationary time series](http://www.insightsbot.com/blog/1MH61d/augmented-dickey-fuller-test-in-python)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhCUFQQMOvKU",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "## Making Time Series Stationary\n",
        "There are 2 major reasons behind non-stationaruty of a TS:\n",
        "\n",
        "1. __Trend__ – varying mean over time\n",
        "\n",
        "2. __Seasonality__ – variations at specific time-frames. eg people might have a tendency to buy cars in a particular month because of pay increment or festivals.\n",
        "\n",
        "Our dataset had no seasonality, but suffers from a downward and an upward trend, which needs to be corrected in order for forecasting to be possible\n",
        "\n",
        "[Other sources to check](https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaR0KuzpOvKV",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "### Transformations\n",
        "\n",
        "- We can apply transformation which penalize higher values more than smaller values\n",
        "- These can be taking a log, square root, cube root\n",
        "\n",
        "#### Log Scale Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89OqV8FAOvKV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np7P1iwdOvKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}