{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wgova/time_series_trade/blob/master/clustering_tsfresh_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "epYv_wB7gafk"
   },
   "outputs": [],
   "source": [
    "!pip install -q dtw-python\n",
    "# dynamic time warping\n",
    "from dtw import *\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import fcluster, ward, dendrogram\n",
    "\n",
    "from scipy.cluster.vq import kmeans,vq\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,normalize\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "from sklearn import decomposition\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import KMeans, SpectralClustering,DBSCAN \n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for automating clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for dendrograms\n",
    "# given a linkage model, plog dendogram, with the colors indicated by the a cutoff point at which we define clusters\n",
    "#https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "def plot_dendrogram(model,type_of_data, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    plt.figure(figsize=(10,5.5))\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "    plt.savefig(f\"Dendrogram_{type_of_data}\")\n",
    "    return linkage_matrix\n",
    "\n",
    "# Functions for dtw\n",
    "def get_dtw_diff_matrix(df,cols:list):\n",
    "    \"\"\"\n",
    "    From a list of series, compute a distance matrix by computing the \n",
    "    DTW distance of all pairwise combinations of series.\n",
    "    \"\"\"\n",
    "    diff_matrix = {}\n",
    "    cross = itertools.product(cols, cols)\n",
    "    for (col1, col2) in cross:\n",
    "        series1 = df[col1]\n",
    "        series2 = df[col2]\n",
    "        diff = dtw(\n",
    "            series1, \n",
    "            series2,\n",
    "            keep_internals=True, \n",
    "            step_pattern=rabinerJuangStepPattern(2, \"c\")\n",
    "            )\\\n",
    "            .normalizedDistance\n",
    "        diff_matrix[(col1, col2)] = [diff]\n",
    "    return diff_matrix\n",
    "\n",
    "def plot_dtw(df,series1:str, series2:str) -> None:\n",
    "  dtw_df = dtw(df[series1],\\\n",
    "            df[series2],\\\n",
    "            keep_internals=True,\n",
    "            step_pattern=rabinerJuangStepPattern(2, \"c\"))\n",
    "  plt.figure(figsize=(10,5.5))\n",
    "  dtw_df.plot(type=\"twoway\",offset=5)\n",
    "  plt.set_title=(f'{series1} and {series2}')\n",
    "  plt.savefig(f\"DTW_{series1}_{series1}\")\n",
    "  plt.show()\n",
    "\n",
    "def plot_elbow_silhoutte_k_evaluation(name_of_data: str,data_array,max_clusters):\n",
    "  range_n_clusters = range(2,max_clusters)\n",
    "  elbow = []\n",
    "  s_score = []\n",
    "  for n_clusters in range_n_clusters:\n",
    "    clusterer = KMeans(n_clusters = n_clusters, random_state=42,init='k-means++',max_iter=1000,n_init=1)\n",
    "    cluster_labels = clusterer.fit_predict(data_array)\n",
    "    # Average silhouette score\n",
    "    silhouette_avg = silhouette_score(data_array, cluster_labels)\n",
    "    s_score.append(silhouette_avg)\n",
    "    # Average SSE\"\n",
    "    elbow.append(clusterer.inertia_) # Inertia: Sum of distances of samples to their closest cluster center\n",
    "    \n",
    "  fig = plt.figure(figsize=(10,5.5))\n",
    "  fig.suptitle(f\"K-means clusters for {name_of_data}\", fontsize=16)\n",
    "  fig.add_subplot(121)\n",
    "  plt.plot(range_n_clusters, elbow,'b-',label=f'{name_of_data} SSE')\n",
    "  plt.xlabel(\"Number of cluster\")\n",
    "  plt.ylabel(\"Sum of squared error(SSE)\")\n",
    "  plt.legend()\n",
    "  \n",
    "  fig.add_subplot(122)\n",
    "  # plt.title(\"Covid-19 Rt values silhouette method results\")\n",
    "  plt.plot(range_n_clusters, s_score,'b-',label=f'{name_of_data} \\n Silhouette Score')\n",
    "  plt.xlabel(\"Number of cluster\")\n",
    "  plt.ylabel(\"Silhouette Score\")\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def plot_kmeans_clusters(data_array,number_of_clusters,name_of_data:str):\n",
    "  # computing K-Means with K = number_of_clusters\n",
    "  centroids,_ = kmeans(data_array,number_of_clusters)\n",
    "  # assign each sample to a cluster\n",
    "  idx,_ = vq(data_array,centroids)\n",
    "  # some plotting using numpy's logical indexing\n",
    "  fig, ax = plt.subplots(figsize=(10,5.5))\n",
    "  fig.suptitle(f\"K-means clusters for {name_of_data}\", fontsize=12)\n",
    "  for cluster in range(number_of_clusters):\n",
    "    colours = ['ob','ok','or','og','om','oc','oy']\n",
    "    ax.plot(data_array[idx==cluster,0],data_array[idx==cluster,1],colours[cluster],label=f'cluster {cluster}')\n",
    "    plt.legend()\n",
    "  plt.savefig(f\"{PATH}/images/k_means_{name_of_data}\")\n",
    "  plt.show()\n",
    "  return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_location = f'{PATH}/mean_scaled_products/'\n",
    "files = os.listdir(scaled_data_location)\n",
    "files[0]\n",
    "yarn_fiber = pd.read_csv(f'{scaled_data_location}/{files[0]}',parse_dates=['year'],index_col='year')\n",
    "remove_null_values(yarn_fiber)\n",
    "\n",
    "path_to_min_features = f'{PATH}/min_feats_ts'\n",
    "path_to_parameter_eff_features = f'{PATH}/efficient_parameters'\n",
    "path_to_parameter_comp_features = f'{PATH}/comprehensive_parameters'\n",
    "min_feats = os.listdir(path_to_min_features)\n",
    "yarn_fiber_min_feats = pd.read_csv(f'{path_to_min_features}/{min_feats[0]}',index_col='id')\n",
    "remove_null_values(yarn_fiber_min_feats)\n",
    "\n",
    "# Exclude highly correlated features\n",
    "exclude = [\n",
    "          #  'export_val__kurtosis',\n",
    "           'export_val__variance',\n",
    "          #  'export_val__mean',\n",
    "           'export_val__skewness',\n",
    "          #  'export_val__standard_deviation',\n",
    "          #  'export_val__median',\n",
    "           'export_val__sum_values',\n",
    "           'export_val__maximum',\n",
    "          #  'export_val__minimum',\n",
    "           'export_val__length'\n",
    "           ]\n",
    "\n",
    "min_feats_yarn = yarn_fiber_min_feats[yarn_fiber_min_feats\\\n",
    "                                      .columns[~yarn_fiber_min_feats\\\n",
    "                                               .columns\\\n",
    "                                               .isin(exclude)]]\n",
    "print(check_outliers(min_feats_yarn))\n",
    "yarn_fiber_min_feats = removing_outliers(min_feats_yarn)\n",
    "\n",
    "#TODO: https://stats.stackexchange.com/questions/427327/simple-outlier-detection-for-time-series\n",
    "min_feats_yarn = min_feats_yarn.div(min_feats_yarn.mean(axis=0),axis=1)\n",
    "product_excl_countries = yarn_fiber_min_feats[yarn_fiber_min_feats.index.isin(random_countries)]\n",
    "product_by_countries = min_feats_yarn[min_feats_yarn.index.isin(random_countries)]\n",
    "from scipy.stats.mstats import winsorize\n",
    "scaler = StandardScaler()\n",
    "scaled_yarn_min_feats = scaler.fit_transform(yarn_fiber_min_feats)\n",
    "\n",
    "# TODO: create function that loops through list columns to be included in  \n",
    "X_scaled_transposed = preprocessing.scale(\n",
    "    np.asarray([\n",
    "                np.asarray(min_feats_yarn['export_val__minimum']),\n",
    "                np.asarray(min_feats_yarn['export_val__mean']),\n",
    "                # np.asarray(min_feats_yarn['export_val__length']),\n",
    "                # np.asarray(min_feats_yarn['export_val__skewness']),\n",
    "                np.asarray(min_feats_yarn['export_val__kurtosis']),\n",
    "                np.asarray(min_feats_yarn['export_val__median']),\n",
    "                # np.asarray(min_feats_yarn['export_val__mean']),\n",
    "                np.asarray(min_feats_yarn['export_val__standard_deviation']),\n",
    "                # np.asarray(min_feats_yarn['export_val__variance'])\n",
    "                ])).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,5.5))\n",
    "fig.add_subplot(121)\n",
    "sns.heatmap(min_feats_yarn.corr(),annot=True)\n",
    "# plt.title(\"Correlation between time series features\")\n",
    "plt.savefig(f\"{PATH}/images/correlation_stats_features\")\n",
    "\n",
    "fig.add_subplot(122)\n",
    "plt.plot(product_excl_countries,label=\"Here\")\n",
    "# plt.title(\"Yarn fiber features for randomly picked \\n countries excluding outliers\")\n",
    "plt.xticks(rotation=70)\n",
    "plt.ylabel(\"Export value\")\n",
    "plt.legend(product_excl_countries.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering mean scaled time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiearchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AgglomerativeClustering(n_clusters=None, distance_threshold = 0).fit(yarn_fiber.T.values)\n",
    "\n",
    "# plot the top five levels of the dendrogram\n",
    "plt.figure(figsize = (10,5.5))\n",
    "plt.title=('Raw data hierarchical Clustering Dendrogram')\n",
    "linkage_matrix = plot_dendrogram(clf, \"raw_data\",p=5, color_threshold = 110,truncate_mode='level')\n",
    "plt.savefig(\"raw_data_hierarchical\")\n",
    "plt.show()\n",
    "# extract clusters from dendogram\n",
    "clusters = fcluster(linkage_matrix, 100, criterion='distance')\n",
    "# create a lookup table for series in a given cluster\n",
    "yarn_fiber_clusters = yarn_fiber.T.reset_index()\n",
    "yarn_fiber_clusters[\"cluster\"] = clusters\n",
    "yarn_fiber_clusters.rename(columns={'index':'country'},inplace=True)\n",
    "yarn_fiber_clustered = yarn_fiber_clusters.set_index(\"cluster country\".split())\\\n",
    "    .sort_index()\n",
    "\n",
    "# cluster analysis\n",
    "clusters = yarn_fiber_clusters.cluster.unique()\n",
    "print(clusters)\n",
    "for c in clusters:\n",
    "  countries= yarn_fiber_clustered.loc[c].index.get_level_values(0).unique()\n",
    "  # random.seed(1)\n",
    "  n_samples = yarn_fiber_clustered.loc[c].shape[0]\n",
    "  if n_samples > 10:\n",
    "    n = random.sample(range(n_samples),10)\n",
    "  else:\n",
    "    n = range(n_samples)\n",
    "  cluster = yarn_fiber_clustered.loc[c].T\n",
    "  cluster.iloc[:, n].plot(subplots=False,figsize = (10,5.5),title=f\"cluster_{c}\")\n",
    "  plt.legend(countries)\n",
    "  # plt.savefig(f\"{PATH}/images/raw_data_cluster{c}\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic time warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 50 series, and compute the DTW distance matrix\n",
    "random.seed(10)\n",
    "sample_cols = random.sample(list(yarn_fiber.columns), 100)\n",
    "dtw_diff_dict = get_dtw_diff_matrix(yarn_fiber,sample_cols)\n",
    "\n",
    "# make into a df\n",
    "dtw_diff_df = pd.DataFrame(dtw_diff_dict).T.reset_index()\\\n",
    "    .rename(columns = {\"level_0\":\"First_variable\", \"level_1\":\"Second_variable\", 0:\"diff\"})\\\n",
    "    .pivot_table(index = \"First_variable\", columns = \"Second_variable\", values = \"diff\")\n",
    "\n",
    "# plot a similarity matrix, with a dendogram imposed\n",
    "import seaborn as sns\n",
    "sns.clustermap(1-dtw_diff_df)\n",
    "\n",
    "# ward clustering from difference matrix, where distance is Dynamic time warping distance instead of Euclidean\n",
    "time_warp = ward(dtw_diff_df)\n",
    "# extract clusters\n",
    "dtw_clusters = pd.DataFrame({\"cluster\":fcluster(time_warp, 1.15)}, index = dtw_diff_df.index)\n",
    "dtw_clusters.cluster.value_counts().sort_index().plot.barh()\n",
    "plt.title=(\"Frequency of DTW clusters\")\n",
    "\n",
    "# Check time series for any cluster\n",
    "# TODO: Function to loop through all clusters and plot\n",
    "# What cluster is South Africa? \n",
    "#print(dtw_clusters[dtw_clusters.index=='South Africa'])\n",
    "cluster = 1\n",
    "yarn_hc_clusters = yarn_fiber.T.merge(\n",
    "    dtw_clusters.loc[dtw_clusters.cluster ==cluster], \n",
    "    left_index = True,\n",
    "    right_index = True)\\\n",
    "    .T\n",
    "yarn_hc_clusters.plot(subplots=True,figsize = (10,5.5),sharey=True,title=f'Countries in cluster {cluster}')\n",
    "plt.show()\n",
    "\n",
    "print('DTW for Turkey and India')\n",
    "plot_dtw(yarn_fiber,\"Turkey\", \"India\")\n",
    "\n",
    "print('DTW for Rwanda and Montenegro')\n",
    "plot_dtw(yarn_fiber,\"Rwanda\", \"Montenegro\")\n",
    "\n",
    "print('DTW for Niger and Republic of the Congo')\n",
    "plot_dtw(yarn_fiber,\"Niger\", \"Republic of the Congo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering TSFRESH extracted features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering TS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_clf = AgglomerativeClustering(n_clusters=None, distance_threshold = 0).fit(min_feats_yarn.values)\n",
    "\n",
    "plt.figure(figsize = (14,6))\n",
    "# plot the top five levels of the dendrogram\n",
    "linkage_matrix = plot_dendrogram(feats_clf,\"min_feats_raw\", p=3,color_threshold = 110,truncate_mode='level')\n",
    "plt.show()\n",
    "\n",
    "# extract clusters from dendogram\n",
    "clusters = fcluster(linkage_matrix, 100, criterion='distance')\n",
    "# create a lookup table for series in a given cluster\n",
    "yarn_fiber_clusters = min_feats_yarn.reset_index()\n",
    "yarn_fiber_clusters[\"cluster\"] = clusters\n",
    "clusts = yarn_fiber_clusters['cluster'].unique()\n",
    "print(f'Unique clusters: {clusts}')\n",
    "yarn_fiber_clusters.rename(columns={'id':'country'},inplace=True)\n",
    "yarn_fiber_clustered = yarn_fiber_clusters.set_index(\"cluster country\".split())\\\n",
    ".sort_index()\n",
    "\n",
    "\n",
    "# cluster analysis\n",
    "feats_clusters = yarn_fiber_clusters.cluster.unique()\n",
    "for c in feats_clusters:\n",
    "  countries= yarn_fiber_clustered.loc[c].index.get_level_values(0).unique()\n",
    "  random.seed(1)\n",
    "  n_samples = yarn_fiber_clustered.loc[c].shape[0]\n",
    "  if n_samples > 10:\n",
    "    n = random.sample(range(n_samples),10)\n",
    "  else:\n",
    "    n = range(n_samples)\n",
    "  cluster = yarn_fiber_clustered.loc[c].T\n",
    "  cluster.iloc[:, n].plot(subplots=True,figsize=(10,5.5),\n",
    "                          title=f\"Countries in cluster_{c}_min_feats\")\n",
    "  plt.xticks(rotation=70)\n",
    "  plt.legend(countries)\n",
    "  plt.savefig(f\"{PATH}/images/cluster_{c}_min_feats\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_elbow_silhoutte_k_evaluation(\"yarn_scaled_min_feats\",X_scaled_transposed,10)  \n",
    "clusters_yarn_min_feats = plot_kmeans_clusters(X_scaled_transposed,6,\"yarn_min_features\") #TODO: seperate def get_clusters() \\\\ plot_kmeans_clusters()\n",
    "\n",
    "#TODO: function for getting names from cluster\n",
    "# def get_cluster_elements()\n",
    "details = [(name,cluster) for name, cluster in zip(min_feats_yarn.index,clusters_yarn_min_feats)]\n",
    "cluster_df = pd.DataFrame(details,columns=['names','cluster'])\n",
    "cluster_df['names'].astype('category')\n",
    "get_names = min_feats_yarn.reset_index().rename(columns={'id':'names'})\n",
    "get_names.names.astype('category')\n",
    "country_cluster = pd.merge(get_names,cluster_df,how='inner', on='names')\n",
    "metrics = ['mean']\n",
    "groups = country_cluster.groupby(['cluster']).agg(metrics)\n",
    "groups.plot(figsize=(15,5.5),kind='bar')\n",
    "country_cluster[country_cluster.cluster==2]['names'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA + k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data to have a mean of ~0 and a variance of 1\n",
    "\n",
    "# TODO: PCA plots and evaluation\n",
    "# Create a PCA instance: pca\n",
    "# def calculate_pca():\n",
    "  # return pca_components_df\n",
    "\n",
    "X_std = StandardScaler().fit_transform(min_feats_yarn)\n",
    "pca = PCA(n_components=5)\n",
    "principalComponents = pca.fit_transform(X_std)\n",
    "\n",
    "# def plot_pca_eveluation():\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5.5))\n",
    "fig.suptitle(\"Clusters for yarn min_features\", fontsize=16)\n",
    "fig.add_subplot(131)\n",
    "plt.bar(features, pca.explained_variance_ratio_, color='black')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %')\n",
    "plt.xticks(features)\n",
    "# Save components to a DataFrame\n",
    "PCA_components = pd.DataFrame(principalComponents)\n",
    "fig.add_subplot(132)\n",
    "plt.scatter(PCA_components[0], PCA_components[1], alpha=.4, color='green')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "\n",
    "\n",
    "plot_elbow_silhoutte_k_evaluation(\"yarn_scaled_pca_min_feats\",np.asarray(PCA_components),10)  \n",
    "clusters_yarn_min_feats = plot_kmeans_clusters(np.asarray(PCA_components),5,\"yarn_min_features_pca\")\n",
    "\n",
    "details = [(name,cluster) for name, cluster in zip(min_feats_yarn.index,clusters_yarn_min_feats)]\n",
    "cluster_df = pd.DataFrame(details,columns=['names','cluster'])\n",
    "cluster_df['names'].astype('category')\n",
    "get_names = min_feats_yarn.reset_index().rename(columns={'id':'names'})\n",
    "get_names.names.astype('category')\n",
    "country_cluster = pd.merge(get_names,cluster_df,how='inner', on='names')\n",
    "metrics = ['mean']\n",
    "groups = country_cluster.groupby(['cluster']).agg(metrics)\n",
    "groups.plot(figsize=(15,5.5),kind='bar')\n",
    "plt.savefig('clusters_yarn_min_features_pca_kmeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/34611038/grid-search-for-hyperparameter-evaluation-of-clustering-in-scikit-learn\n",
    "!pip install -q clusteval\n",
    "# Import library\n",
    "from clusteval import clusteval\n",
    "# Set parameters, as an example dbscan\n",
    "ce = clusteval(method='dbscan')\n",
    "# Fit to find optimal number of clusters using dbscan\n",
    "results= ce.fit(X_scaled_transposed)\n",
    "# Make plot of the cluster evaluation\n",
    "plt.figure(figsize=(10,5.5))\n",
    "ce.plot()\n",
    "# Make scatter plot. Note that the first two coordinates are used for plotting.\n",
    "ce.scatter(X_scaled_transposed)\n",
    "# results is a dict with various output statistics. One of them are the labels.\n",
    "cluster_labels = results['labx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DBSCAN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5e92219951ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# https://www.dummies.com/programming/big-data/data-science/how-to-create-an-unsupervised-learning-model-with-dbscan/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdbscan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdbscan_results_yarn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbscan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled_transposed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbscan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DBSCAN' is not defined"
     ]
    }
   ],
   "source": [
    "# https://www.dummies.com/programming/big-data/data-science/how-to-create-an-unsupervised-learning-model-with-dbscan/\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5,metric='euclidean')\n",
    "dbscan_results_yarn = dbscan.fit(X_scaled_transposed)\n",
    "print(np.unique(dbscan.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.dummies.com/programming/big-data/data-science/how-to-create-an-unsupervised-learning-model-with-dbscan/\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5,metric='euclidean')\n",
    "dbscan.fit(X_scaled_transposed)\n",
    "fig = plt.figure(figsize=(10,5.5))\n",
    "fig.suptitle(\"Clusters for PCA-DBSCAN : yarn min_features\", fontsize=16)\n",
    "print(np.unique(dbscan.labels_))\n",
    "fig.add_subplot(121)\n",
    "# fig.set_title('Clusters')\n",
    "pca = PCA(n_components=2)\n",
    "pca_2d = pca.fit_transform(X_scaled_transposed)\n",
    "for i in range(0, pca_2d.shape[0]):\n",
    "  if dbscan.labels_[i] == 0:\n",
    "    c1 = plt.scatter(pca_2d[i,0],pca_2d[i,1],c='r',marker='+')\n",
    "  elif dbscan.labels_[i] == 1:\n",
    "    c2 = plt.scatter(pca_2d[i,0],pca_2d[i,1],c='g',marker='o')\n",
    "  elif dbscan.labels_[i] == 2:\n",
    "    c3 = plt.scatter(pca_2d[i,0],pca_2d[i,1],c='b',marker='*')\n",
    "  elif dbscan.labels_[i] == 3:\n",
    "    c4 = plt.scatter(pca_2d[i,0],pca_2d[i,1],c='y',marker='x')\n",
    "  elif dbscan.labels_[i] == 4:\n",
    "    c5 = plt.scatter(pca_2d[i,0],pca_2d[i,1],c='g',marker='*')\n",
    "  elif dbscan.labels_[i] == 5:\n",
    "    c6 = plt.scatter(pca_2d[i,0],pca_2d[i,1],c='r',marker='x')\n",
    "  elif dbscan.labels_[i] == -1:\n",
    "    c7 = plt.scatter(pca_2d[i,0],pca_2d[i,1],c='c',marker='+')\n",
    "plt.xlabel(\"Number of clusters_dbscan\")\n",
    "plt.legend([c1, c2, c3,c4,c5,c6,c7], ['Cluster 1', \n",
    "                                'Cluster 2',\n",
    "                                'Cluster 3',\n",
    "                                'Cluster 4',\n",
    "                                'Cluster 5',\n",
    "                                'Cluster 6',\n",
    "                                'Noise'])\n",
    "\n",
    "# plt.title('DBSCAN finds 2 clusters and noise')\n",
    "fig.add_subplot(122)\n",
    "# plt.set_title=(\"Cluster instances/frequency\")\n",
    "plt.hist(dbscan.labels_,bins=8)\n",
    "plt.savefig(f\"{PATH}/images/yarn_min_feats_pca_dbscan\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM3tTmSQEHT3yjbmtVwJ4bq",
   "include_colab_link": true,
   "mount_file_id": "1S90IW_oJPrZCTTvnduSLu1B-bbb8xtt0",
   "name": "clustering_tsfresh_features.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
