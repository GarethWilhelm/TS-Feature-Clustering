{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clustering_full_time_series.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMwmnuF6FR2TPwkJYxxhBnb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wgova/time_series_trade/blob/master/notebooks/clustering_full_time_series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXq1u4_DI5Bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q dtw-python\n",
        "# dynamic time warping\n",
        "from dtw import *\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import fcluster, ward, dendrogram\n",
        "\n",
        "from scipy.cluster.vq import kmeans,vq\n",
        "from math import sqrt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler,normalize\n",
        "from sklearn.metrics.cluster import homogeneity_score\n",
        "from sklearn import decomposition\n",
        "from scipy.stats.mstats import winsorize\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from sklearn.cluster import KMeans, SpectralClustering,DBSCAN \n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3T0PCM4Jjea",
        "colab_type": "text"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG4pebz-JihH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions for dtw\n",
        "def get_dtw_diff_matrix(df,cols:list):\n",
        "    \"\"\"\n",
        "    From a list of series, compute a distance matrix by computing the \n",
        "    DTW distance of all pairwise combinations of series.\n",
        "    \"\"\"\n",
        "    diff_matrix = {}\n",
        "    cross = itertools.product(cols, cols)\n",
        "    for (col1, col2) in cross:\n",
        "        series1 = df[col1]\n",
        "        series2 = df[col2]\n",
        "        diff = dtw(\n",
        "            series1, \n",
        "            series2,\n",
        "            keep_internals=True, \n",
        "            step_pattern=rabinerJuangStepPattern(2, \"c\")\n",
        "            )\\\n",
        "            .normalizedDistance\n",
        "        diff_matrix[(col1, col2)] = [diff]\n",
        "    return diff_matrix\n",
        "\n",
        "def plot_dtw(df,series1:str, series2:str) -> None:\n",
        "  dtw_df = dtw(df[series1],\\\n",
        "            df[series2],\\\n",
        "            keep_internals=True,\n",
        "            step_pattern=rabinerJuangStepPattern(2, \"c\"))\n",
        "  dtw_df.plot(type=\"twoway\",offset=5)\n",
        "  plt.savefig(f\"DTW_{series1}_{series1}\")\n",
        "  plt.show()\n",
        "\n",
        "# Functions for dendrograms\n",
        "# given a linkage model, plog dendogram, with the colors indicated by the a cutoff point at which we define clusters\n",
        "#https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
        "def plot_dendrogram(raw_ts_dataframe,name_of_dataset, **kwargs):\n",
        "    model = AgglomerativeClustering(n_clusters=None, distance_threshold = 0).fit(raw_ts_dataframe.T.values)\n",
        "    # Create linkage matrix and then plot the dendrogram\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
        "                                      counts]).astype(float)\n",
        "\n",
        "    # Plot the corresponding dendrogram\n",
        "    fig = plt.figure(figsize=(10,5.5))\n",
        "    fig.suptitle(f'Hierarchical clusters: {name_of_dataset}')\n",
        "    dendrogram(linkage_matrix, **kwargs)\n",
        "    plt.savefig(f\"hierarchical_{name_of_dataset}\")\n",
        "    return linkage_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSIUYXjNJ6yP",
        "colab_type": "text"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8odvlPwJ6En",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ref = 0\n",
        "PATH = '/content/drive/My Drive/Stellenbosch/Webster'\n",
        "path_to_parameter_eff_features = f'{PATH}/efficient_parameters'\n",
        "path_to_parameter_comp_features = f'{PATH}/comprehensive_parameters'\n",
        "list_files_min_feats = os.listdir(path_to_parameter_eff_features)\n",
        "list_files_eff_features = os.listdir(path_to_parameter_eff_features)\n",
        "scaled_data_location = f'{PATH}/mean_scaled_products/'\n",
        "# files = os.listdir(min_feats)\n",
        "data = list_files_eff_features[ref]\n",
        "print(data)\n",
        "product_name = 'engines_misc'\n",
        "\n",
        "product_df = pd.read_csv(f'{path_to_parameter_eff_features}/{data}',index_col='id')#,parse_dates=['year'],index_col='year')\n",
        "product_df_no_nulls = remove_null_values(product_df)\n",
        "# print(check_outliers(uncorrelated_product_df))\n",
        "product_df_no_outliers = removing_outliers(product_df_no_nulls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsthUuHPKL6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler()\n",
        "# scaler.fit_transform(uncorrelated_product_df_no_outliers)\n",
        "product_scaled_uncorrelated = pd.DataFrame(scaler.fit_transform(features_uncorrelated),\n",
        "                              columns = features_uncorrelated.columns,\n",
        "                              index=features_uncorrelated.index)\n",
        "len(product_scaled_uncorrelated.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1wfyjAmKO8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO: https://stats.stackexchange.com/questions/427327/simple-outlier-detection-for-time-series\n",
        "random.seed(20)\n",
        "random_countries = sample(list(features_uncorrelated.index),5)\n",
        "product_excl_countries = features_uncorrelated[features_uncorrelated.index.isin(random_countries)]\n",
        "product_by_countries = features_uncorrelated[features_uncorrelated.index.isin(random_countries)]\n",
        "X_scaled_transposed = np.asarray(features_uncorrelated)\n",
        "X_scaled_transposed.shape\n",
        "\n",
        "fig = plt.figure(figsize=(16,5.5))\n",
        "fig.add_subplot(121)\n",
        "sns.heatmap(features_uncorrelated.corr(),annot=True)\n",
        "# plt.title(\"Correlation between time series features\")\n",
        "plt.savefig(f\"{PATH}/images/correlation_stats_features\")\n",
        "\n",
        "fig.add_subplot(122)\n",
        "plt.plot(features_uncorrelated,label=\"Here\")\n",
        "# plt.title(\"Yarn fiber features for randomly picked \\n countries excluding outliers\")\n",
        "plt.xticks(rotation=70)\n",
        "plt.ylabel(\"Export value\")\n",
        "# plt.legend(features_uncorrelated.columns)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKTs3IyLLepv",
        "colab_type": "text"
      },
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mL7Z_i4JZoY",
        "colab_type": "text"
      },
      "source": [
        "## Hierarchical clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2lXVnpDJZoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the top five levels of the dendrogram\n",
        "plt.figure(figsize = (10,5.5))\n",
        "linkage_matrix = plot_dendrogram(yarn_mean_scaled, \"yarn_fiber_full_data\",p=3, color_threshold = 110,truncate_mode='level')\n",
        "plt.savefig(\"raw_data_hierarchical\")\n",
        "plt.show()\n",
        "# extract clusters from dendogram\n",
        "clusters = fcluster(linkage_matrix, 100, criterion='distance')\n",
        "# create a lookup table for series in a given cluster\n",
        "yarn_fiber_clusters = yarn_mean_scaled.T.reset_index()\n",
        "yarn_fiber_clusters[\"cluster\"] = clusters\n",
        "yarn_fiber_clusters.rename(columns={'index':'country'},inplace=True)\n",
        "yarn_fiber_clustered = yarn_fiber_clusters.set_index(\"cluster country\".split())\\\n",
        "    .sort_index()\n",
        "\n",
        "# cluster analysis\n",
        "clusters = yarn_fiber_clusters.cluster.unique()\n",
        "print(clusters)\n",
        "for c in clusters:\n",
        "  countries= yarn_fiber_clustered.loc[c].index.get_level_values(0).unique()\n",
        "  # random.seed(1)\n",
        "  n_samples = yarn_fiber_clustered.loc[c].shape[0]\n",
        "  if n_samples > 10:\n",
        "    n = random.sample(range(n_samples),10)\n",
        "  else:\n",
        "    n = range(n_samples)\n",
        "  cluster = yarn_fiber_clustered.loc[c].T\n",
        "  cluster.iloc[:, n].plot(subplots=False,figsize = (10,5.5),title=f\"yarn_cluster_{c}\")\n",
        "  plt.legend(countries)\n",
        "  plt.savefig(f\"{PATH}/images/yarn_full_ts_cluster{c}\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWoUfuZ_JZod",
        "colab_type": "text"
      },
      "source": [
        "## Dynamic time warping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcwYkHyDJZoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample 50 series, and compute the DTW distance matrix\n",
        "random.seed(15)\n",
        "sample_cols = random.sample(list(yarn_mean_scaled.columns), 20)\n",
        "sample_cols.append('South Africa')\n",
        "# sample_cols.extend(['South)\n",
        "dtw_diff_dict = get_dtw_diff_matrix(yarn_mean_scaled,sample_cols)\n",
        "\n",
        "# make into a df\n",
        "dtw_diff_df = pd.DataFrame(dtw_diff_dict).T.reset_index()\\\n",
        "    .rename(columns = {\"level_0\":\"First_variable\", \"level_1\":\"Second_variable\", 0:\"diff\"})\\\n",
        "    .pivot_table(index = \"First_variable\", columns = \"Second_variable\", values = \"diff\")\n",
        "\n",
        "# plot a similarity matrix, with a dendogram imposed\n",
        "sns.clustermap(1-dtw_diff_df,figsize=(10,5.5))\n",
        "\n",
        "# ward clustering from difference matrix, where distance is Dynamic time warping distance instead of Euclidean\n",
        "time_warp = ward(dtw_diff_df)\n",
        "# extract clusters\n",
        "dtw_clusters = pd.DataFrame({\"cluster\":fcluster(time_warp, 1.15)}, index = dtw_diff_df.index)\n",
        "dtw_clusters.cluster.value_counts().sort_index().plot.barh()\n",
        "plt.title=(\"Frequency of DTW clusters\")\n",
        "\n",
        "# Check time series for any cluster\n",
        "# TODO: Function to loop through all clusters and plot\n",
        "# What cluster is South Africa? \n",
        "#print(dtw_clusters[dtw_clusters.index=='South Africa'])\n",
        "cluster = 1\n",
        "yarn_hc_clusters = yarn_mean_scaled.T.merge(\n",
        "    dtw_clusters.loc[dtw_clusters.cluster ==cluster], \n",
        "    left_index = True,\n",
        "    right_index = True)\\\n",
        "    .T\n",
        "yarn_hc_clusters.plot(subplots=True,figsize = (10,5.5),sharey=True,title=f'Countries in cluster {cluster}')\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(10,5.5))\n",
        "fig.suptitle(f'DTW : Examples')\n",
        "# print('DTW for Turkey and India')\n",
        "plot_dtw(yarn_mean_scaled,\"Turkey\", \"India\")\n",
        "print('DTW for Rwanda and Montenegro')\n",
        "plot_dtw(yarn_mean_scaled,\"Rwanda\", \"Montenegro\")\n",
        "# fig.add_subplot(133)\n",
        "print('DTW for Niger and Republic of the Congo')\n",
        "plot_dtw(yarn_mean_scaled,\"Niger\", \"Republic of the Congo\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjU9w3mqK8cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.dummies.com/programming/big-data/data-science/how-to-create-an-unsupervised-learning-model-with-dbscan/\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5,metric='euclidean')\n",
        "dbscan_results_yarn = dbscan.fit(X_scaled_transposed)\n",
        "print(np.unique(dbscan.labels_))\n",
        "\n",
        "# https://www.dummies.com/programming/big-data/data-science/how-to-create-an-unsupervised-learning-model-with-dbscan/\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5,metric='euclidean')\n",
        "dbscan.fit(X_scaled_transposed)\n",
        "fig = plt.figure(figsize=(10,5.5))\n",
        "fig.suptitle(\"Clusters for PCA-DBSCAN : yarn min_features\", fontsize=16)\n",
        "print(np.unique(dbscan.labels_))\n",
        "fig.add_subplot(121)\n",
        "# fig.set_title('Clusters')\n",
        "pca = PCA(n_components=2)\n",
        "pca_2d = pca.fit_transform(X_scaled_transposed)\n",
        "for i in range(0, pca_2d.shape[0]):\n",
        "  if dbscan.labels_[i] == 0:\n",
        "    c1 = plt.scatter(pca_2d[i,0],pca_2d[i,1],c='r',marker='+')\n",
        "  elif dbscan.labels_[i] == 1:\n",
        "    c2 = plt.scatter(pca_2d[i,0],pca_2d[i,1],c='g',marker='o')\n",
        "  elif dbscan.labels_[i] == 2:\n",
        "    c3 = plt.scatter(pca_2d[i,0],pca_2d[i,1],c='b',marker='*')\n",
        "  elif dbscan.labels_[i] == 3:\n",
        "    c4 = plt.scatter(pca_2d[i,0],pca_2d[i,1],c='y',marker='x')\n",
        "  elif dbscan.labels_[i] == 4:\n",
        "    c5 = plt.scatter(pca_2d[i,0],pca_2d[i,1],c='g',marker='*')\n",
        "  elif dbscan.labels_[i] == 5:\n",
        "    c6 = plt.scatter(pca_2d[i,0],pca_2d[i,1],c='r',marker='x')\n",
        "  elif dbscan.labels_[i] == -1:\n",
        "    c7 = plt.scatter(pca_2d[i,0],pca_2d[i,1],c='c',marker='+')\n",
        "plt.xlabel(\"Number of clusters_dbscan\")\n",
        "plt.legend([c1, c2, c3], ['Cluster 1', \n",
        "                                'Cluster 2',\n",
        "                                'Cluster 3',\n",
        "                                'Cluster 4',\n",
        "                                'Cluster 5',\n",
        "                                'Cluster 6',\n",
        "                                'Noise'])\n",
        "\n",
        "# plt.title('DBSCAN finds 2 clusters and noise')\n",
        "fig.add_subplot(122)\n",
        "# plt.set_title=(\"Cluster instances/frequency\")\n",
        "plt.hist(dbscan.labels_,bins=8)\n",
        "plt.savefig(f\"{PATH}/images/yarn_min_feats_pca_dbscan\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}